

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\input{mydef}
\begin{document}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Weight Block Sparsity: Training, Compilers, and Accelerators  }

\author{
  \IEEEauthorblockN{Taehee Jeong}  
  \and
  \IEEEauthorblockN{Akshay Jain}  
  %\and
  %\IEEEauthorblockN{Shreyas Manjunath}  
  %\and
  %\IEEEauthorblockN{Mrinal Sarmah}  
  %\and
  %\IEEEauthorblockN{Samuel Hsu}
  %\and
  %\IEEEauthorblockN{Nitesh Pipralia}
  \and
  \IEEEauthorblockN{ Paolo D'Alberto}
}

%\renewcommand{\shortauthors}{D'Alberto, et al.}
\maketitle

\begin{abstract}
Inference is a synonymous for performance: for example, more and
faster classifications per seconds. As faster and tailor-made
chip-lets are deployed in the field also larger and larger models are
domineering the public's attention: quantization and sparsification
are used to fit these large models into more reasonable sized
ones. Quantization reduces the foot print per model weight and
sparsification trims superfluous ones.  We present the main ideas
about a vertical system where convolution and matrix multiplication
weights can be trained to exploit an 8x8 block sparsity, compilers
recognize such a sparsity for data compaction and computation
splitting into threads and cores.

Blocks present spatial locality that a vector operation can make full
use and temporal locality for good reuse and cost amortization. If we
take a Resnet50, we can reduce the weight by half with little accuracy
loss, We can achieve speeds similar to an hypothetical Resnet25. We
shall present performance estimates by accurate and complete code
generation for a small and efficient set of AIE2 (Xilinx Versal
FPGAs). We show application for general CPU, GPUS.

\end{abstract}


\begin{IEEEkeywords}
Sparsity, Performance, FPGA, Tools
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}

We shall start by what we mean for block sparsity and for a vertical
solution. This will provide a clear introduction to the subject. Block
sparsity is an intuitive concept but it is also a little misunderstood
because we all have a different mental picture of it. Take a matrix
multiplication as in Equation \ref{eq:mat}
\begin{equation}
  \label{eq:mat}
  \begin{pmatrix}
    \Vc{C}_0 & \Vc{C}_1 \\
    \Vc{C}_2 & \Vc{C}_3 \\ 
  \end{pmatrix} = 
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 \\
    \Vc{A}_2 & \Vc{A}_3 \\ 
  \end{pmatrix}\\
  \begin{pmatrix}
    \Vc{0}   & \Vc{B}_1 \\
    \Vc{B}_2 & \Vc{0} \\ 
  \end{pmatrix}\\
\end{equation}
This is simply the computation
{\small \begin{equation}
  \Vc{C}_0 = \Vc{A}_{1} \Vc{B}_{2}; \;
  \Vc{C}_1 = \Vc{A}_{0} \Vc{B}_{1}; \;
  \Vc{C}_2 = \Vc{A}_{3} \Vc{B}_{2}; \;
  \Vc{C}_3 = \Vc{A}_{2} \Vc{B}_{1}
\end{equation}}
and in general with proper $\gamma_i$
\begin{equation}
  \Vc{C}_i = \sum_{k=0}^1 \Vc{A}_{2i+ k} \big(\gamma_{2*k+i} \Vc{B}_{2*k+i}\big)
\end{equation}


Where the matrix $\Vc{B}$ is a constant and diagonal and each
submatrices $\Vc{B_2}$ and $\Vc{B}_1$ can split further down and show
zero blocks of even smaller sizes. In this work, we chose the basic
block of being of size $\Vc{B}_i = 8\times 8$. It is arbitrary but it
is a great starting point for architectures based on AMD AIE2
products.  For example,
\begin{equation}
  \Vc{B} = \dot{\sum}_i \gamma_i \Vc{B}_i, \;\; \gamma_i \in \{0,1\} 
\end{equation}
Each block is a constant either non zero or zero:
{\small \begin{equation*}
  \Vc{B} = 
  \begin{bmatrix}
    \gamma_0\Vc{B}_0   &  ... &   \gamma_0\Vc{B}_{n-1} \\
    \gamma_n\Vc{B}_{n} &  ... &   \gamma_{2n-1}\Vc{B}_{2n-1} \\
    ...               &  ... &   ... \\
    \gamma_{(n-1)n}\Vc{B}_{(n-1)n} & ... &   \gamma_{(n-1)^2}\Vc{B}_{(n-1)^2} \\
  \end{bmatrix}
\end{equation*}}
Of course, the matrix $\Vc{A}$ can have any value and so
$\Vc{C}$. Some applications have some constraints about the row and
columns of $\Vc{B}_i$: for example, each row or column cannot be
zero. In practice, we do not prune the network, we keep the same
number of {\em channels} every where.

This is a well known data structure in the sparse computation field:
We can use Compress Block Row or Column format (CBR). There are
standard Matrix Sparse-Matrix Multiplication interfaces and algorithms
for CPU and GPUs using this data format (where only one operand is
sparse).

In the CBR format, the $\gamma_i$ are not present and only non zeros
elements are stored. In other architectures, we can choose to store
all non zero blocks in row format and keep a matrix $\Gamma$ of zeros
and ones (or columns). The $\Gamma$ is a bit matrix (here) but it can
be represented as a short integer matrix representing the non-zero
block column and in general two orders of magnitude ($8\times 8 \times
4$) smaller than the sparse or original $\Vc{B}$ matrix. It is
possible the problem has inherent block sparsity. In general, we are
working with {\em man made} block sparsity: indeed for us, the
granularity of the dense block is a property of the hardware.

In classification, the model weight size determines one important
aspect of the model complexity: the number of operations per single
output, the relation between layers (e.g., depth), and redundancy. The
last property is important for resiliency and also to leave some space
to learn new features if necessary. Exploiting sparsity is another way
to reduce redundancies and computations. In our context, sparsity is
the zeroing of weights (convolution and fully connected): we start
with dense model using full precision and we have to find a way to
chose the binary matrix $\Gamma$ (which is also called a mask). For a
matrix multiplication the mask is $\Gamma \in \R^{n\times m}$. In a
convolution, $n$ is the number of output channels (divided by 8) and
$m$ is the number of input channels (as above) and each $\Vc{B}_i \in
\R^{8\times h \times w \times 8}$ where $h$ and $w$ are the height and
width of the convolution kernel. Imagine $\Vc{B}$ having a dimension
going into the paper of size $k \times w$.

We explore training techniques (PyTorch, Keras is available in
github): the most successful so far is the simplest: we take a
pre-trained model, we compute a $\Gamma$ per layer using a function to
determine the blocks more likely to be zeros (Norm) and then we train
the model till convergence or accuracy achieved. We take the sparse
model and we quantize to 8-bit integer computation by using Vitis-AI
quantizer. The final model is a XIR quantize model (assuming addition
accumulators of 32 bits). See Section \ref{sec:training}.

For FPGA accelerator using AIE2, we have a custom compiler that takes
the XIR model and an abstraction of a connected set of AIE2. See
Section \ref{sec:compiler}. For example, A DDR, one or two memory
dedicated per column (512KB each called Mem-tile), 1 to 8 columns, 1
to 8 AIE2 cores per column, and each core has 8*8KB internal
memory. There are vertical connections and there are horizontal
connections. Given the HW and per layer, the compiler computes the
maximum sub-volume computation per core. By heuristics and following a
schedule, it computes a memory allocation in mem-tile for input,
outputs, and weights. It formats the weights so that to exploit
spatial distribution to Mem-tiles and cores into a compact form into a
single tensor of weight, bias, and $\Gamma$.

With the schedule and the DDR-MemTile allocation, we generate all the
explicit communications between DDR, MemTile, and cores. Knowing the
subproblem sizes per core and the computation throughput and with a
clear specification of what is executed in parallel: we can estimate
the execution time per layer and of the entire network with an
accuracy closer to a simulation. The code is valid, the native AIE2
compiler can interpret it, with all the DMA settings in place, and we
tested for a single core in HW.  We shall use this code to have a
detailed time estimates for all parts of the computation: we shall
show estimates for two CNN models, a few different AIE designs; see
Section \ref{sec:experiments}.

In the following Section \ref{sec:motivation}, we shall start with a
quantitative measure about the advantages of block sparsity in
general.

\section{Block-Sparse Matrix-Matrix Multiplication}
\label{sec:motivation}

 As mental and practical exercise, consider $\Gamma$ and
$\Omega$ two appropriate 0,1 matrices so that for square matrices in
$\R^{N\times N}$
\begin{equation}
  \Vc{C} = (\Gamma \Vc{A}) * (\Omega \Vc{B})
\end{equation}
More precisely, consider non-zero blocks of size $k\times k$ so that
\begin{equation}
  \Vc{C}_{i*N+j} = \sum_k ( \gamma_{i*N+k} \Vc{A}_{i*N+k} ) (\omega_{k*N+j} \Vc{B}_{k*N+j})
\end{equation}
Thanks to the sparsity and if we store only non-zeros, then
$gamma_{i*N+k}$ is at the very least contiguous but $\omega_{k*N+j}$
and the right operand accesses are far from being neither simple not
contiguous.
\begin{equation}
  \dot{\Omega}\dot{\Vc{B}} = (\Omega \Vc{B})^t =\Omega^t \Vc{B}^t 
\end{equation}
Although expensive, the transposition of a sparse matrix is a sorting
algorithm: We start from a row order to a column order, then consider
$K$ non-zeros, $K\log_2(K)$. 

\begin{equation}
  \Vc{C}_{i*N+j} = \sum_k ( \gamma_{i*N+k} \Vc{A}_{i*N+k} ) (\dot{\omega}_{j*N+k} \dot{\Vc{B}}_{j*N+k})
\end{equation}
There will be a meaningful product to compute if and only if
$\gamma_{i*N+k} =1$, $\dot{\omega}_{j*N+k} =1$, and
$i*N+k=j*N+k$. Without any extra information, this is simply merge
sort. If we take $\gamma_{i*N+k} = i*N+k$ when the relative block is
non zero, nothing otherwise, create a vector, and do the same for
$\dot{\omega}_{j*N+k}$, then we merge-sort these vectors, we do
computations only on equality: we have to inspect each non zero
elements $M_0$ and $M_1$, which is $\leq O(\frac{N}{K})$. If you like
to break codes yourself, see how the Sparse Sparse Matrix
multiplication using Coordinate Block Structure (COO) is, please go
play with \cite{PaoloG2020}. We have a parallel sorting and a parallel
matrix multiplication.

Now, the intuitive part, assume we want to achieve a fixed sparsity
(i.e., density) of 50\% for a square matrix of size $N$ and we choose
the block size $k \times k$. The number of blocks per dimension and
thus the overhead for sparsity and sorting, is basically
$\frac{1}{2}\frac{N}{k}$. Larger the $k$ smaller the overhead.  The
relative performance of the $k^3$ multiplication is better as $k$ get
larger because spatial and temporal locality and optimized code for a
constant/parameterized $k$.

\doublefigure{0.99}{1x1.png}{8x8.png}{Block 1x1 and 8x8
  performance}{fig:block} In Figure \ref{fig:block}, we present two
scatter plots: on the abscissa number of effective multiplication and
addition, on the ordinate the performance in GFLOPS, when the sparsity
dense block is 1x1 and 8x8. Given the same problem, we may use more
threads and thus the Jenga of points.  We can see that given he same
number of effective operations, the block permits better performance
and exploits better performance for each precisions: see a 2{$\times$}
performance for single precision computation versus double
precision. This is a sparse computation and thus the poor
peformance (in GFLOPS) is actually expected (the peak performance in
this architecture is about 500+GFLOPS).




\section{Block Sparsity: Training and Quantization}
\label{sec:training}

In Convolutional Neural Networks, the two main operations are
convolutions/correlations and fully connected layers (matrix
multiplication). The block sparsity we are seeking to deploy is not
naturally recurring and it has to be made. We must train the network
so that we can zero blocks of data.

First, let us clarify block sparsity for convolution weights, then we
clarify our training process. A convolution has a weight tensor in
four dimension: $\Vc{W} \in \R^{c_{out}\times h \times k \times
  c_{in}}$. If you can visualize the $h$ and $k$ dimension going into
the paper: We can simplify the weight as $\dot{\Vc{W}} \in \R^{c_{out}
  \times c_{in}}$ and block sparsity can be simply described by a mask
$\Gamma\dot{\Vc{W}}$. Although, we speak of a $8\times 8$ of non
zeros, this is in practice a $8\times h\times k\times 8$ block. For
the matrix multiply $h=k=1$ and there is no difference from the
previous discussions.



\subsection{Keras}
We provide a repository using Keras \cite{chollet2015keras} where we
implements the contents of this section: Any one can reproduce and
break \cite{PaoloK2020}. We target convolutions only and without
quantization. The idea of the framework is simple: we take any model
and we create a copy where we enhance the convolution with a
(non-trainable) $\Gamma$. A convolution will have three parameters
(saving the model into a different format).  The forward computation
is modified so that the weights used for convolution are
$\Gamma\Vc{W}$. We assume the backward computation (i.e., gradient) is
done automatically from the forward definition. There is no need to
change the bias. For example, we take Resnet50 from the keras
application repository, we start with a $\Gamma=1$, and we trained one
epoch using imagenet repository \cite{deng2009imagenet}.  The goal is
to choose $\Gamma$ so that we achieve the required sparsity and to
have the minimum loss in accuracy. A little notation first:

We start from an optimized network and assume a loss function ${\bf
  \ell}(x,w)$.  In a close interval of the optimal solution $\Vc{w}_0$
we have:
\begin{equation}
  \label{eq:loss}
  {\bf \ell}(x,\Vc{w}_0 +\Triangle{\Vc{w})} = {\bf \ell}(x,\Vc{w}_0) +
  \nabla{\bf \ell}*\Triangle{\Vc{w}} + (\Triangle{\Vc{w}})^t *H*
  (\Triangle{\Vc{w}}) + \epsilon
\end{equation}
Where the gradient of the loss function is about zero and defined as
\begin{equation}
  \nabla{\bf \ell}* \Triangle{\Vc{w}} = \sum_{w_i}
  \frac{\partial\ell}{\partial w_i} \Triangle{w_i} \rightarrow 0
\end{equation}
The second component is the Hessian, it is symmetric and either
definite positive or negative as a function of the loss.
\begin{equation}
  (\Triangle{\Vc{w}})^t *H*(\Triangle{\Vc{w}}) = \sum_{w_i}
  \Triangle{w_i} \sum_{w_j} \frac{\partial\ell}{\partial w_i\partial
    w_j} \Triangle{w_j}
\end{equation}
We start from $\Vc{w}_0$, the current optimal weight, and we must
choose how to reduce to zeros the weight and which ones.

In practice, using the code available in \cite{PaoloK2020}, we started
with a accuracy of 75\% top-1 and we ended up with a 68\% without
accounting for quantization. The investigation using PyTorch, sparsity
and quantization achieves only 3\% top-1 accuracy drop (instead of
6+), making the training for sparsity quite worth while (Section \ref{sec:pytorch}).



\subsection{$\Gamma$ chosen once and full training ahead}
\label{sec:one-mask}
Take a convolution with $\Gamma = 1$ and weights $\Vc{W}$. For each
$\gamma_i$, this will be representative of a block $\Vc{W}_i \in \R^{8
  \times h \times w \times 8} \sim \R^{8\times 8}$. We can choose the
$\Vc{W}_i$ using a measure of importance:
\begin{itemize}
  \item $L_2 = \sqrt{\sum_k w_k^2}$ with $w_k$ elements of $\Vc{W}_i$
  \item $L_1 = \sum_k |w_k|$ with 
  \item Variance $\sigma^2 = \frac{1}{64}\sum_k (w_k -\mu)^2$ with
    $\mu = \frac{1}{64}\sum w_k $ or $\frac{1}{N}\sum w_k$ (the whole
    tensor). In signal processing $\sigma^2$ is the power of the
    signal.
\end{itemize}
We can then sort them in ascending order and for example choose the
first 50\% to set to zero. Then we start re-training. We do this for the
whole network or for one convolution at a time. This is the approach
used in Section \ref{sec:pytorch}

\subsection{$\Gamma$ chosen in steps  and small fine tuning}

Let say that for every convolution, $n_i =\sum \gamma_i$, which is the
number of blocks. We would like to reduce this number to
$\frac{n_i}{2}$. For each epoch (say every two training epochs), we
consider the current non-set-yet mask elements $\sum \gamma_i = k <
\frac{n_i}{2}$. We compute our importance measure for all in ascending
order. This time, we zero the first $\min(\frac{5}{100}n_i, 1)$. We
keep this process until we reach 50\% sparsity. At each iteration at
least one block will be set to zero.

\subsection{$\Gamma$ trainable as optimization problem}

We think it is worth mentioning: If we want to make $\Gamma$ part of
the optimization process as trainable variable we could introduce as a
penalty function into the loss ${\bf \ell}(x,w) + \lambda(w)$.

First let us introduce an approximation for the $\max(x)$, so when in
this section you will read $\max, \min$, this is a log
sum exponetial,
which is continuous, derivable everywhere, and convex:
\begin{equation}
  \max(\Vc{x}) = LSE(\Vc{x},\alpha) = \frac{1}{\alpha}\log \sum e^{x_i*\alpha} 
\end{equation}
With $T$ we represent the number of non zero block in $\Gamma$
\begin{flalign}
  \lambda=  &  -(\max(\Gamma)- \min(\Gamma))  &&\\\nonumber
  & +\beta*L2(\Gamma-T) + \iota*L1(\Gamma)  &&
\end{flalign}

This is a simplified loss so that we minimize the valu of $\Gamma$ but
also we try to maximize the difference of the elements.

\begin{flalign}
  \lambda=  & \max(-\Gamma,0) + \max(\Gamma -1, 0)  -(\max(\Gamma)- \min(\Gamma)) &&\\\nonumber
            & + \beta*L2(\Gamma-T) + \iota*L1(\Gamma) &&
\end{flalign}

This last penalty function represents our attempt to state the
$\gamma_ i$ should be positive and in the interval $[0,1]$ and in a
such a way that we maximize the distance of the elements between 0s
and 1s.
\begin{flalign}
  \lambda=  & \max(-\Gamma,0) + \max(\Gamma -1, 0)  -\frac{\min(\Gamma)}{\max(\Gamma)} &&\\\nonumber
            &+ \beta*L2(\Gamma-T) + \iota*L1(\Gamma)   
\end{flalign}

We can exploit this functionality in Keras: the penalty function can
be added quite easily and per convolution (if you like) and it is
available in the code reference. We could not use it successfully: we
do not know if this is because the lack of the penalty it self, the
optimizer, or the lost function, or just our own fault.

\subsection{Hessian and Fisher Information }
If we have $N$ parameters/weights, the Hessian $H \in R^{N\times N}$
has quite the space complexity (consider even small models could have
million parameters). When we are already close to the optimal solution
or we are trying to move from the optimal solution without using
information from the gradient, the Hessian provides the most
information close by an already established solution point. There are
also way to compute the Hessian and the effect of the hessian by using
Fisher information \cite{yao2020adahessian,abs-2101-08940,zandonati2022fit} and this will reduce to the
computation of the gradient of the loss function and the citation
within.  We could not reproduce the authors results.

The goal is to use the $H$ so that we know in advance what will be the
effect of zeroing weights. This will boil down to a $O(h_i*w_i)$ that
we can use to estimate the importance of a block

\subsection{Diagonal Hessian}
We applied a Fisher measure and we just computed $\nabla^2{\bf \ell}$,
that is we compute just the diagonal of the Hessian. Again, we use the
$L_2$ over the normalized weight and went thought the process of
training. The elements of the diagonal are not representative in
general, but they are a good approximation of the contribution of a
single weight. Clearly, convolution is linear and any CNN is a deep
network.

The Fisher and $\nabla^2{\bf \ell}$ did not provide any main
advantages. But this information is found very useful in quantization
and optimizations within the same field and application. We embrace the
community to help us to understand this better. 

\subsection{PyTorch} 
\label{sec:pytorch}


In Table \ref{tab_acc}, we show the results by using one-time masks
and full training: VGG-16, ResNet-50, Inceptionv3 on ImageNet20 (20
classes) and ImageNet1k (1000 classes). See Section
\ref{sec:one-mask}. We use three samples per class for the validation
accuracy for ImageNet1k data set; instead, we use 50 samples per class
for ImageNet20. Fine-tuning sparse networks on the original ImageNet
data-set \cite{deng2009imagenet} is expensive. To reduce the training
time, we chose 20 classes (from the original 1000 classes) with the
least number of images per class in the training data-set and this
choice will affect the accuracy because there are fewer sample for
re-training.


\begin{table}[ht]
\caption{Accuracies of the sparsity models}
\label{tab_acc}
\begin{center} 
\scalebox{0.9}
{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rule[-1ex]{0pt}{3.5ex}  Model & Dataset & Baseline  & \multicolumn{3}{c|}{Sparsity}\\
\rule[-1ex]{0pt}{3.5ex}  {} & {} & Acc.(\%) & block & ratio (\%) & Acc.(\%)    \\\hline\hline
\rule[-1ex]{0pt}{3.5ex}  Inception-v3 & ImageNet1k & 77.2 & 8x8 & 50 & 75.5  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet1k & 76.7 & 8x8 & 50 & 74.6  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet1k & 70.6 & 8x8 & 50 & 69.7  \\\hline \hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 8x8 & 25 & 95.1  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 8x8 & 50 & 92.0  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 8x8 & 75 & 87.1  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 1x1 & 25 & 96.0  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 1x1 & 50 & 95.6  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 1x1 & 75 & 93.5  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet20 & 92.0 & 8x8 & 50 & 89.6  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet20 & 92.0 & 1x1 & 50 & 92.3  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet20 & 92.0 & 1x1 & 75 & 91.7  \\\hline
\end{tabular}\vspace{-20pt}
}
\end{center}
\end{table}

Classification accuracy on ImageNet1k drops by only 1 - 2\% after
applying 50\% sparsity with a $8\times 8$ block (this is without any
quantization). We experiment with different block shapes such as
$16\times 4$ and $4\times 16$ on ResNet-50, but the accuracy is
slightly worse compared to block of $8\times 8$. Fine-grained sparsity
($1\times 1$ block) does not sacrifice any accuracy (i.e., almost
any).  We use the sparsified Resnet50, we quantized using Vitis AI,
and we shall use it for estimate the time advantages (i.e., Section
\ref{sec:experiments}).


\section{Compiler and Code generation AIE}
\label{sec:compiler}
Within our Xilinx/AMD effort, we can take a PyTorch/Keras, quantize it
using Vitis AI, and create an intermediate representation by using
what we call Xilinx Intermediate Representation (XIR). This is a graph
of operations that read and write tensors. A convolution has one
quantized input: we use the layout format BHWC, the tensors are
represented in INT8 with a position where the fraction starts (power
of two scale). It computes a tensor using the same layout as the input
and with a proper scale. The weights and bias are properties of the
convolutions, as such they can be tailored.

For this project, we are at the third generation of a compiler for
custom architectures (previously DPUV1 and DPUV3int8
\cite{10.1145/3473334,abs-2110-04327}). The main common harwdware
feature for this new architecture is the availability of two main
memory levels: DDR and MemTile, and for such memory we import the
philosophy of memory data layout and memory allocation (of the
previous compilers).

All weights are statically prepared into DDR and we move them
explicitly towards the inner levels. Inputs and outputs have
designated space DDR (to communicate externally with PCI connection or
RAM). DDR can and it will be used for swap tensors in case the inner
levels do not allow allocation.  The compiler can explore different
schedules: the memory allocation to Mem-Tile is basically coloring
algorithm plus some heuristics. In this architecture, we do not allow
{\em streaming} of neither data or weights (because they share space
in Mem-Tile). In previous architecture, we could.

\subsection{AIE Hardware abstraction}

\singlefigure{0.90}{AIE.png}{4x4 AIE representation}{fig:aie}

For this presentation, see Figure \ref{fig:aie}, we work with a mesh
of 4x4 AIE2 cores connected by 4 horizontal and 4 vertical
interconnections. Each core has 8 banks memories for a total
64KB. About 6 banks are used and input/output/weight buffers and 2
banks are used as temporary space for kernels. Each core can request
and send data to its direct neighbors (if aware of connection and
control).

There are four mem-tiles: each 512KB and each can either be connected
to one columns and its direct neighbor column, or it can be connected
to a row and its neighbor. The total amount of space is 2MB.

A Memtile can broad-cast data per column or per row. We can dedicate a
memtile for weights, one for activations, or we can share. To maximize
the computation parallelism, the core will write data per column into
memtile: there are different designs that can be used. We shall
explore the case where Inputs/Outputs/Weights are evenly distributed
across mem-tiles although the distribution is different per tensor.

The DDR is connected with two channels to write into each mem-tile and
each mem-tile can use two channels to write into DDR. DDR and Mem-tile
communications are parallel.  The abstraction can be extended to more
complex connections (from $1\times 1$, $8\times 2$, to $8\times 8$ and
more rows of memtiles designed for larger chips). But $4\times 4$ is
representative for the AIE engines you will find in the next
generation of CPU+FPGA chips.


\subsection{Sub Volume, Data compression and Data Movement}
There are a few assumptions. The computation is split by column: each
output tensor is split evenly by width, so each memtile will store
different columns by width, each core will compute different output
channels, and the computation streams computing the height of the
tensor by using core input and output ping/pong. As often as possible,
we store the weights in the core and we reuse them as much as possible
(unless we need to stream the weight instead). The set of cores is a
cohort and symmetric computations are always selected. This means that
we do not merge two operations like convolution and max-pool and give
three columns to convolution and one column to max-pool. Other
approach in the literature will do this more advanced methodology.

Before we start describing the memory allocation, we have to explain
and compute the size of the sub-volume computation at AIE core.  That
is, if we have the inputs, output, and weights in memtile, what is the
largest computation we can do in the AIE? The minimum computation is
at least one output channel and one row by height. If this is not
possible, we try to reduce the size of the width (e.g., shaping the
tensor in memtile by using DDR spils) and we can manage to split the
input channels (this will require to split the weights accordingly and
accumulate). We call W-Split the distribution of tensor by columns in
the AIE mesh. We can CIN-split when we need to split by input channel.
The split of the computation is also called tiling. Once we have the
subvolume, we know if input and output tensor need W-split or
CIN-split. This in general, means that the tensor cannot fit
completely in Mem-tile and part of it will be moved. Or the
computation will need to read from mem-tile the same inputs or weights
multiple times.

At this stage, the subvolume describes the smallest shape of the
weights we need to manage. We compress the weight accordingly to such
a subvolume so that any data movement will always be a multiple of the
subvolume and can be a single load. Such a compress data will have the
same properties whether is sparse or not. Of course, sparse data is
smaller and we compute fewer operations.


\subsection{Schedule and memory allocation}
This is a two level memory hierarchy. During the scheduling of each
layer, we evaluate what can fit in mem tile. Here, Activations and
weights share the space. At each step the memory allocation will check
if we can allocate a tensor in memtile. If we cannot, we evict all
tensors into DDR and then split/time the computation. If we can, we
do, and we will evict the tensors no longer needed accordingly.

At the end of this stage, every tensor will be associated to an
address in memtile or DDR (or both). If there are only DDR addresses,
the compiler will take the basic computation and, by an heuristics,
will split the computation (thus the output tensor) by width, output
channel, height, and input channel (non necessarily in this
order). Every computation will have the DDR to and from MemTile and
its data movements for the first two levels of the memory hierarchy.

We use a implementation that takes a recursive and iterative approach
in tiling \cite{abs-2110-04327}: For clarity, $\sum$ is a sum or
reduction and $\dot{\sum}$ is a parallel loop and a W-split can be
written as
\begin{equation}
  \Vc{Y} =  Conv(\Vc{X},\Vc{W}) = \dot{\sum}_{w=1}^k
  Conv(\Vc{B}_w,\Vc{W})
\end{equation}
The split is pre-computed as function of the foot print, before and
after each convolution there will be an explicit data movement. At
this stage each input, output, and weights will have an address
associated with each sub-computation. Then the code generation of each
$Conv(\Vc{B}_w,\Vc{W})$ is independent and recursive and as needed
there will be specific split of the computation accordingly. This
could be simply described using loops but it is actually a tree (i.e,
a root with $k$ children). If the convolution has strides and large
kernel, each sub-convolution may have overlap data but defined
addresses and data movement if necessary. For example, computing the
output by rows and the weights are reused.
\begin{equation}
  \forall w, \;   Conv(\Vc{B}_w,\Vc{W}) \rightarrow  \dot{\sum}_h Conv(\Vc{B}_{w,h},\Vc{W})
\end{equation}


\subsection{Code Generation }
The compiler recursively creates a list of operations smaller and
smaller that can actually be executed Mem-Tile to Mem-Tile. In
practice, there is a further decomposition using only AIE cores but it
is completely determined by the subvolume computation we did
previously. We can explicitly determine the data movements from
mem-tile to core and back.

A peculiar feature of the current system is the concept of {\em
  iteration}. Using locks and chaining them (locks like in
semaphores): We can create a single iteration instruction from the
prospective of a single core (as a SIMD instruction) but driving all
cores at once (ASM-like code) {\footnotesize
\begin{verbatim}
  LOAD Lock k_0 mem-tile add core add iter i
  CONV iteration      i
  SAVE Lock k_1 mem-tile add core add iter i
\end{verbatim}
} There is an implicit lock (say \verb2k_x2) that is used for the pong
and the system cycles in between locks (\verb2k_x2 and \verb2k_02)


These three operations will be execute a number of iterations {\em i}
and using a ping/pong they will load different slices of data and
compute different slices of data. The key ingredient for this to work
is the volumes of input and output tensors are all the same. 

In our environment padding is common and we can manage by a custom
load (from memtile to core) and this requires a custom load that will
not be repeated
{\footnotesize
\begin{verbatim}
  ## Head top padding
  LOAD Lock k_0 mem-tile add_0 core add iter 1
  CONV iteration 1
  SAVE Lock k_1 mem-tile add_1 core add iter 1
  ## Body iteration
  ## k_0 -> k_2 -> k_4 Lock Chain
  LOAD Lock k_2 mem-tile add_2 core add iter 7
  CONV iteration 7
  SAVE Lock k_3 mem-tile add_3 core add iter 7
  ## tail
  LOAD Lock k_4 mem-tile add_4 core add iter 1
  CONV iteration 1
  SAVE Lock k_5 mem-tile add_5 core add iter 1
\end{verbatim}
 }




At this stage we have all the information: the code generation per
layer is a two pass process: first, we generate code for the all
load/store and then we combine them into chain having dependency so
that to be logically correct and as fast as possible. Take the code
above and we introduce a chain information. 

\singlefigure{0.95}{AIE4x4.png}{Resnet50 for 4x4 AIE with dense
  weights}{fig:estimate-dense}


\subsection{Time Estimation}
At this stage, we need to explain how we can capture the execution
time and visualize it as in Figure \ref{fig:estimate-dense}.

\subsection{Time estimates for DDR to Mem-Tile}
We have two communication types: activations and weights. Per
mem-tile there are two dedicated channels.
\begin{itemize}
 \item if we share activations and weights in the same mem-tile (we
   use the moniker WAWAWAWA), we can use one channel for activations
   and one for weights. Thus the loads from DDR to MEM-tile (LOAD and
   LOADW) are parallel with a bandwidth of 4GB/s. Writes from mem-tile
   to DDR (WRITE) can use both channels (8GB/s). We shall expand this.

 \item If activations and weights go to different mem-tiles (2 and 2),
   each load is parallel and 8GB/s (we use the moniker WAAW). Writes
   are identical.
\end{itemize}
Although, mem-tile are circular buffers to improve the memory
allocation, the classic streaming is not really applicable for two
main reasons: First, when we share weights they must stay still to be
reused, if weights are not shared then in general inputs and outputs
have different rates of consumption and thus eventually one will
overwrite the other. Ping/pong techniques will decrease the space
available but it will hide latency: at this time we try to increase
the reuse.
   
\subsection{Time estimates for Mem-Tile to AIEs}

The Memtile connections with AIE cores can be designed differently as
a function of the purpose. We assume here a few channels with again
4GB/s bandwidth. One memetile can broad cast inputs to columns of
cores (and to the nearest neighbrs). These communications are for
activations (LOADFM). One Memtile can broadcast to rows of cores (or
the nearest neighbor), these are for weights (LOADWM). We assume that
the column and row communications are parallel and each memtile core
connection is parallel.

Every communication with iteration 1 is synchronous and binding: that
is sequential, the load, convolution, and store is one after the other
and every core is independent.  For synchronization and for
bookkeeping, we assume that AIE2 weights communication (from memtile)
to core are synchronous and halting.

Every communication with iteration larger than one, we assume that
load, computation (COMP), and store (WRITEFM) are executed in parallel
and the overall execution time is the maximum of the estimated time
multiplied by the number of iterations.

We estimate the execution time of a subvolume by the number of
operations divided by the maximum number of operations per cycle which
is in our scenario: $4\times 8 \times 8 = 256 $ operations per cycle
and 1GHz frequency.

We do not account the wait and synchronization necessary to reprogram
the fabric.

During and especially once we generated the code for all data
movements, data volumes and destinations, the trigger of the
computation, and the sub volume per computation we can estimate quite
accurately the execution time for each operations and account their
parallel execution.

\subsection{Convolution example}
\doublefigure{0.999}{singledenseconv.png}{singlesparseconv.png}{Resnet
  single convolution with padding and sparsity for 4x4 AIE
}{fig:singleconvestimate} Here and in Figure
\ref{fig:singleconvestimate}, we give a full convolution example with
and without sparsity and with padding. In this way we can explain how
the time are really estimated and also how the hardware works in
principle.

It is clear from the Figure that there are three computations
(COMP). We load the weight and activations once in memtiles. There are
actually one load per memtile for a total of 4 load per activations
and for loads for weights. Because each load is to a different
memtile, they are parallel.  The activation and weights communications
are using two different channels and then are in parallel with 4GB/s
bandwidth.

There is a single load of the weights from mem-tiles to each
core. This is done once and it is blocking (LOADWM). There is a
computation using padding (top-padding) and you can see the sequential
execution of load to cores (LOADFM), computation (COMP), and write to
memtile (WRITEFM). This computation has iteration 1. There there are 9
iterations instructions: You can see the computation, load and write
in parallel. This is obviously a simplifcation; there will be a little
load poking out at the beginning and a writing poking out at the end.
Then we conclude with the final computation with padding at the bottom
of the computation.

In this particular scenario, the computation dominates the execution
time and compression is basically cut by half: from 200 $\mu s$ to
130. There are convolutions in Resnet that realize exactly $2\times$
performance but also there are convolution that are dominated by the
read or by the writing, and where sparsity help only in space saving.

\section{Results}
\label{sec:experiments}
In this section, we present the performance of sparsity applied to all
the convolutions (but the first one) for Resnet 50 Figure
\ref{fig:estimate-dense} and \ref{fig:estimate-sparse}, and Inception
V3 Figure \ref{fig:incv3-estimate-dense}.

When we generate the code for each instruction, we compute the
execution time. So if we inspect the assembly code we will find time
information in the context whether or not each instruction contribute
directly. For data movement to and from DDR and mem-tile, we reduce
the contribution (sum directly) for the other information we create a
time series and we create eventually a json file we use for
visualization and then a javascript we can visualize the time series
using a browser.

Even if the images are small, we can see that 50\% sparsity achieves
about 30\% improvement in performance. The main reason is that the
mem-tile is still too small and we cannot keep all tensors in it and
we need to spill into DDR.


\Singlefigure{0.80}{AIE4x4sparse.png}{Resnet50 for 4x4 AIE with 50\%
  sparse weights}{fig:estimate-sparse}





\Doublefigure{0.90}{inceptionv3dense.png}{inceptionv3sparse.png}{Inception V3 for 4x4 AIE
  with dense weights and sparse }{fig:incv3-estimate-dense}



\section{Conclusions}
This is a multifaceted problem and we present a complete solution from
training techniques, compilers, code generatio, HW definition, and
time estimations.


%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{IEEEtran} \bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix{Review and Response}
%\input{review.tex}
\end{document}
