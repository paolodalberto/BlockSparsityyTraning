Name:Sparse Engine #1 arrays
(Shared) DDR    : Name DDR Size 34359738368 Pairing [[0]] 
   Banks [
	Name  0 Layout 5 Unit 8-bits Rows 67108864 Column 64 dict_keys([(0, 34359738368)]) [None, None, None, None, None, None] 
   ]
 - array temp

Per Core Ping (and Pong):
MiscCoreBuffer : 11264-bit InputCoreBuffer : 131072-bit OutputCoreBuffer : 131072-bit WeightCoreBuffer : 131072-bit 
 [(0,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(1,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(2,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(3,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(4,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']

(Combined) WEIGHTS: Name ParameterBuffer Size 20971520 Pairing [[0]] 
   Banks [
Row:
	Name  0 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  1 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  2 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  3 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  4 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
   ]
(Combined) ACT    : Name FeatureMapBuffer Size 20971520 Pairing [[0]] 
   Banks [
Row:
	Name  0 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  1 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  2 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  3 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  4 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
   ]
proto /wrk/xsjhdnobkup5/taeheej/vitis-ai-staging/quant/quantize_result_vgg16/VGG_int.xmodel
caffe None
 ############################################# 
 ######  Hyper Graph Construction 
 ############################################# 
 ############################################# 
 ######  Hyper Graph Construction
 ############################################# 
 ############################################# 
 ######  Architecture Summary
 ############################################# 
Name:Sparse Engine #1 arrays
(Shared) DDR    : Name DDR Size 34359738368 Pairing [[0]] 
   Banks [
	Name  0 Layout 5 Unit 8-bits Rows 67108864 Column 64 dict_keys([(0, 34359738368)]) [None, None, None, None, None, None] 
   ]
 - array temp

Per Core Ping (and Pong):
MiscCoreBuffer : 11264-bit InputCoreBuffer : 131072-bit OutputCoreBuffer : 131072-bit WeightCoreBuffer : 131072-bit 
 [(0,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(0,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(1,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(1,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(2,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(2,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(3,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(3,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [(4,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [(4,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']

(Combined) WEIGHTS: Name ParameterBuffer Size 20971520 Pairing [[0]] 
   Banks [
Row:
	Name  0 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  1 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  2 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  3 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  4 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
   ]
(Combined) ACT    : Name FeatureMapBuffer Size 20971520 Pairing [[0]] 
   Banks [
Row:
	Name  0 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  1 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  2 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  3 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
	Name  4 Layout 4 Unit 8-bits Rows 4096 Column 128 dict_keys([(0, 4194304)]) 
   ]
Floyd & Warshall
BFS
 ############################################# 
 ######  Parameters Assimilation: # 34 
 ############################################# 
  0 data       VGG__input_0 Ops 0 Shape [1, 224, 224, 3]  IN [] OUT ['VGG__input_0_fix']
  1 fix        VGG__input_0_fix Ops 0 Shape [1, 224, 224, 3]  IN ['VGG__input_0'] OUT ['VGG__VGG_Sequential_features__Conv2d_0__input_3']
  2 conv2d     VGG__VGG_Sequential_features__Conv2d_0__input_3 Ops 0 Shape [1, 224, 224, 64]  IN ['VGG__input_0_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_1__input_5']
  3 relu       VGG__VGG_Sequential_features__ReLU_1__input_5 Ops 0 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__Conv2d_0__input_3'] OUT ['VGG__VGG_Sequential_features__ReLU_1__input_5_fix']
  4 fix        VGG__VGG_Sequential_features__ReLU_1__input_5_fix Ops 0 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__ReLU_1__input_5'] OUT ['VGG__VGG_Sequential_features__Conv2d_2__input_7']
  5 conv2d     VGG__VGG_Sequential_features__Conv2d_2__input_7 Ops 0 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__ReLU_1__input_5_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_3__1238']
  6 relu       VGG__VGG_Sequential_features__ReLU_3__1238 Ops 0 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__Conv2d_2__input_7'] OUT ['VGG__VGG_Sequential_features__ReLU_3__1238_fix']
  7 fix        VGG__VGG_Sequential_features__ReLU_3__1238_fix Ops 0 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__ReLU_3__1238'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9']
  8 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_4__input_9 Ops 0 Shape [1, 112, 112, 64]  IN ['VGG__VGG_Sequential_features__ReLU_3__1238_fix'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9_fix']
  9 fix        VGG__VGG_Sequential_features__MaxPool2d_4__input_9_fix Ops 0 Shape [1, 112, 112, 64]  IN ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9'] OUT ['VGG__VGG_Sequential_features__Conv2d_5__input_11']
 10 conv2d     VGG__VGG_Sequential_features__Conv2d_5__input_11 Ops 0 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_6__input_13']
 11 relu       VGG__VGG_Sequential_features__ReLU_6__input_13 Ops 0 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__Conv2d_5__input_11'] OUT ['VGG__VGG_Sequential_features__ReLU_6__input_13_fix']
 12 fix        VGG__VGG_Sequential_features__ReLU_6__input_13_fix Ops 0 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__ReLU_6__input_13'] OUT ['VGG__VGG_Sequential_features__Conv2d_7__input_15']
 13 conv2d     VGG__VGG_Sequential_features__Conv2d_7__input_15 Ops 0 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__ReLU_6__input_13_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_8__1292']
 14 relu       VGG__VGG_Sequential_features__ReLU_8__1292 Ops 0 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__Conv2d_7__input_15'] OUT ['VGG__VGG_Sequential_features__ReLU_8__1292_fix']
 15 fix        VGG__VGG_Sequential_features__ReLU_8__1292_fix Ops 0 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__ReLU_8__1292'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17']
 16 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_9__input_17 Ops 0 Shape [1, 56, 56, 128]  IN ['VGG__VGG_Sequential_features__ReLU_8__1292_fix'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17_fix']
 17 fix        VGG__VGG_Sequential_features__MaxPool2d_9__input_17_fix Ops 0 Shape [1, 56, 56, 128]  IN ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17'] OUT ['VGG__VGG_Sequential_features__Conv2d_10__input_19']
 18 conv2d     VGG__VGG_Sequential_features__Conv2d_10__input_19 Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_11__input_21']
 19 relu       VGG__VGG_Sequential_features__ReLU_11__input_21 Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_10__input_19'] OUT ['VGG__VGG_Sequential_features__ReLU_11__input_21_fix']
 20 fix        VGG__VGG_Sequential_features__ReLU_11__input_21_fix Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__ReLU_11__input_21'] OUT ['VGG__VGG_Sequential_features__Conv2d_12__input_23']
 21 conv2d     VGG__VGG_Sequential_features__Conv2d_12__input_23 Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__ReLU_11__input_21_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_13__input_25']
 22 relu       VGG__VGG_Sequential_features__ReLU_13__input_25 Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_12__input_23'] OUT ['VGG__VGG_Sequential_features__ReLU_13__input_25_fix']
 23 fix        VGG__VGG_Sequential_features__ReLU_13__input_25_fix Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__ReLU_13__input_25'] OUT ['VGG__VGG_Sequential_features__Conv2d_14__input_27']
 24 conv2d     VGG__VGG_Sequential_features__Conv2d_14__input_27 Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__ReLU_13__input_25_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_15__1366']
 25 relu       VGG__VGG_Sequential_features__ReLU_15__1366 Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_14__input_27'] OUT ['VGG__VGG_Sequential_features__ReLU_15__1366_fix']
 26 fix        VGG__VGG_Sequential_features__ReLU_15__1366_fix Ops 0 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__ReLU_15__1366'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29']
 27 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_16__input_29 Ops 0 Shape [1, 28, 28, 256]  IN ['VGG__VGG_Sequential_features__ReLU_15__1366_fix'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29_fix']
 28 fix        VGG__VGG_Sequential_features__MaxPool2d_16__input_29_fix Ops 0 Shape [1, 28, 28, 256]  IN ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29'] OUT ['VGG__VGG_Sequential_features__Conv2d_17__input_31']
 29 conv2d     VGG__VGG_Sequential_features__Conv2d_17__input_31 Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_18__input_33']
 30 relu       VGG__VGG_Sequential_features__ReLU_18__input_33 Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_17__input_31'] OUT ['VGG__VGG_Sequential_features__ReLU_18__input_33_fix']
 31 fix        VGG__VGG_Sequential_features__ReLU_18__input_33_fix Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__ReLU_18__input_33'] OUT ['VGG__VGG_Sequential_features__Conv2d_19__input_35']
 32 conv2d     VGG__VGG_Sequential_features__Conv2d_19__input_35 Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__ReLU_18__input_33_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_20__input_37']
 33 relu       VGG__VGG_Sequential_features__ReLU_20__input_37 Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_19__input_35'] OUT ['VGG__VGG_Sequential_features__ReLU_20__input_37_fix']
 34 fix        VGG__VGG_Sequential_features__ReLU_20__input_37_fix Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__ReLU_20__input_37'] OUT ['VGG__VGG_Sequential_features__Conv2d_21__input_39']
 35 conv2d     VGG__VGG_Sequential_features__Conv2d_21__input_39 Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__ReLU_20__input_37_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_22__1440']
 36 relu       VGG__VGG_Sequential_features__ReLU_22__1440 Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_21__input_39'] OUT ['VGG__VGG_Sequential_features__ReLU_22__1440_fix']
 37 fix        VGG__VGG_Sequential_features__ReLU_22__1440_fix Ops 0 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__ReLU_22__1440'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41']
 38 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_23__input_41 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__ReLU_22__1440_fix'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41_fix']
 39 fix        VGG__VGG_Sequential_features__MaxPool2d_23__input_41_fix Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41'] OUT ['VGG__VGG_Sequential_features__Conv2d_24__input_43']
 40 conv2d     VGG__VGG_Sequential_features__Conv2d_24__input_43 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_25__input_45']
 41 relu       VGG__VGG_Sequential_features__ReLU_25__input_45 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_24__input_43'] OUT ['VGG__VGG_Sequential_features__ReLU_25__input_45_fix']
 42 fix        VGG__VGG_Sequential_features__ReLU_25__input_45_fix Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__ReLU_25__input_45'] OUT ['VGG__VGG_Sequential_features__Conv2d_26__input_47']
 43 conv2d     VGG__VGG_Sequential_features__Conv2d_26__input_47 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__ReLU_25__input_45_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_27__input_49']
 44 relu       VGG__VGG_Sequential_features__ReLU_27__input_49 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_26__input_47'] OUT ['VGG__VGG_Sequential_features__ReLU_27__input_49_fix']
 45 fix        VGG__VGG_Sequential_features__ReLU_27__input_49_fix Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__ReLU_27__input_49'] OUT ['VGG__VGG_Sequential_features__Conv2d_28__input_51']
 46 conv2d     VGG__VGG_Sequential_features__Conv2d_28__input_51 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__ReLU_27__input_49_fix'] OUT ['VGG__VGG_Sequential_features__ReLU_29__1514']
 47 relu       VGG__VGG_Sequential_features__ReLU_29__1514 Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_28__input_51'] OUT ['VGG__VGG_Sequential_features__ReLU_29__1514_fix']
 48 fix        VGG__VGG_Sequential_features__ReLU_29__1514_fix Ops 0 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__ReLU_29__1514'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53']
 49 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_30__input_53 Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__ReLU_29__1514_fix'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53_fix']
 50 fix        VGG__VGG_Sequential_features__MaxPool2d_30__input_53_fix Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53'] OUT ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0']
 51 avgpool2d  VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53_fix'] OUT ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544']
 52 mul        VGG__VGG_AdaptiveAvgPool2d_avgpool__1544 Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0'] OUT ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_fix']
 53 fix        VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_fix Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544'] OUT ['VGG__VGG_1547']
 54 reshape    VGG__VGG_1547 Ops 0 Shape [1, 25088]  IN ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_fix'] OUT ['VGG__VGG_Sequential_classifier__Linear_0__input_55']
 55 matmul     VGG__VGG_Sequential_classifier__Linear_0__input_55 Ops 0 Shape [1, 4096]  IN ['VGG__VGG_1547'] OUT ['VGG__VGG_Sequential_classifier__ReLU_1__input_57']
 56 relu       VGG__VGG_Sequential_classifier__ReLU_1__input_57 Ops 0 Shape [1, 4096]  IN ['VGG__VGG_Sequential_classifier__Linear_0__input_55'] OUT ['VGG__VGG_Sequential_classifier__ReLU_1__input_57_fix']
 57 fix        VGG__VGG_Sequential_classifier__ReLU_1__input_57_fix Ops 0 Shape [1, 4096]  IN ['VGG__VGG_Sequential_classifier__ReLU_1__input_57'] OUT ['VGG__VGG_Sequential_classifier__Linear_3__input_59']
 58 matmul     VGG__VGG_Sequential_classifier__Linear_3__input_59 Ops 0 Shape [1, 4096]  IN ['VGG__VGG_Sequential_classifier__ReLU_1__input_57_fix'] OUT ['VGG__VGG_Sequential_classifier__ReLU_4__input']
 59 relu       VGG__VGG_Sequential_classifier__ReLU_4__input Ops 0 Shape [1, 4096]  IN ['VGG__VGG_Sequential_classifier__Linear_3__input_59'] OUT ['VGG__VGG_Sequential_classifier__ReLU_4__input_fix']
 60 fix        VGG__VGG_Sequential_classifier__ReLU_4__input_fix Ops 0 Shape [1, 4096]  IN ['VGG__VGG_Sequential_classifier__ReLU_4__input'] OUT ['VGG__VGG_Sequential_classifier__Linear_6__1558']
 61 matmul     VGG__VGG_Sequential_classifier__Linear_6__1558 Ops 0 Shape [1, 1000]  IN ['VGG__VGG_Sequential_classifier__ReLU_4__input_fix'] OUT ['VGG__VGG_Sequential_classifier__Linear_6__1558_fix']
 62 fix        VGG__VGG_Sequential_classifier__Linear_6__1558_fix Ops 0 Shape [1, 1000]  IN ['VGG__VGG_Sequential_classifier__Linear_6__1558'] OUT []
 ############################################# 
 ######  Reshape must not have parameters 
 ###### We remove Reshapes into vectors [4,1,1,x] -> [4,x] 
 ############################################# 
remove data VGG__VGG_1547
 ############################################# 
 ######  Assimilating Fix Neurons: # 23 
 ############################################# 
 ############################################# 
 ######  Assimilating Relu: # 15 
 ############################################# 
 ############################################# 
 ######  Assimilating Relu6: # 0 
 ############################################# 
 ############################################# 
 ######  Assimilating LeakyRelu: # 0 
 ############################################# 
 ############################################# 
 ######  BATCH NORM as 1x1 convolution (I Know) 
 ############################################# 
 ############################################# 
 ######  I like VALID more than SAME
 ############################################# 
 ############################################# 
 ######  I like VALID more than SAME: # 0 
 ############################################# 
 ############################################# 
 ######  Fix info into activation Tensors
 ############################################# 
VGG__input_0
VGG__VGG_Sequential_features__Conv2d_0__input_3
VGG__VGG_Sequential_features__Conv2d_2__input_7
VGG__VGG_Sequential_features__MaxPool2d_4__input_9
VGG__VGG_Sequential_features__Conv2d_5__input_11
VGG__VGG_Sequential_features__Conv2d_7__input_15
VGG__VGG_Sequential_features__MaxPool2d_9__input_17
VGG__VGG_Sequential_features__Conv2d_10__input_19
VGG__VGG_Sequential_features__Conv2d_12__input_23
VGG__VGG_Sequential_features__Conv2d_14__input_27
VGG__VGG_Sequential_features__MaxPool2d_16__input_29
VGG__VGG_Sequential_features__Conv2d_17__input_31
VGG__VGG_Sequential_features__Conv2d_19__input_35
VGG__VGG_Sequential_features__Conv2d_21__input_39
VGG__VGG_Sequential_features__MaxPool2d_23__input_41
VGG__VGG_Sequential_features__Conv2d_24__input_43
VGG__VGG_Sequential_features__Conv2d_26__input_47
VGG__VGG_Sequential_features__Conv2d_28__input_51
VGG__VGG_Sequential_features__MaxPool2d_30__input_53
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544
VGG__VGG_1547
VGG__VGG_Sequential_classifier__Linear_0__input_55
VGG__VGG_Sequential_classifier__Linear_3__input_59
VGG__VGG_Sequential_classifier__Linear_6__1558
 ############################################# 
 ######  Assimilating Padding:# 0 
 ############################################# 
Forward CUT Outputs dict_keys(['VGG__VGG_Sequential_classifier__Linear_6__1558'])
True VGG__input_0
True VGG__VGG_Sequential_features__Conv2d_0__input_3
True VGG__VGG_Sequential_features__Conv2d_2__input_7
True VGG__VGG_Sequential_features__MaxPool2d_4__input_9
True VGG__VGG_Sequential_features__Conv2d_5__input_11
True VGG__VGG_Sequential_features__Conv2d_7__input_15
True VGG__VGG_Sequential_features__MaxPool2d_9__input_17
True VGG__VGG_Sequential_features__Conv2d_10__input_19
True VGG__VGG_Sequential_features__Conv2d_12__input_23
True VGG__VGG_Sequential_features__Conv2d_14__input_27
True VGG__VGG_Sequential_features__MaxPool2d_16__input_29
True VGG__VGG_Sequential_features__Conv2d_17__input_31
True VGG__VGG_Sequential_features__Conv2d_19__input_35
True VGG__VGG_Sequential_features__Conv2d_21__input_39
True VGG__VGG_Sequential_features__MaxPool2d_23__input_41
True VGG__VGG_Sequential_features__Conv2d_24__input_43
True VGG__VGG_Sequential_features__Conv2d_26__input_47
True VGG__VGG_Sequential_features__Conv2d_28__input_51
True VGG__VGG_Sequential_features__MaxPool2d_30__input_53
True VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0
True VGG__VGG_AdaptiveAvgPool2d_avgpool__1544
Outputs dict_keys(['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544'])
Floyd & Warshall
BFS
Floyd & Warshall
BFS
 ############################################# 
 ######  CPU nodes Must Go
 ############################################# 
Inputs ['VGG__input_0']
Outputs ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544']
['conv2d', 'maxpool2d', 'add', 'resize', 'upsample', 'downsample', 'reshape', 'squeeze', 'mul', 'inner-product', 'matmul', 'avgpool2d', 'depthwise-conv2d', 'transposed-depthwise-conv2d', 'concat', 'identity', 'transposed-conv2d', 'transposed-upsa2d']
FPGA True: data       VGG__input_0     
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_0__input_3  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_2__input_7  
FPGA True: maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_4__input_9  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_5__input_11  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_7__input_15  
FPGA True: maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_9__input_17  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_10__input_19  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_12__input_23  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_14__input_27  
FPGA True: maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_16__input_29  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_17__input_31  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_19__input_35  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_21__input_39  
FPGA True: maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_23__input_41  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_24__input_43  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_26__input_47  
FPGA True: conv2d     VGG__VGG_Sequential_features__Conv2d_28__input_51  
FPGA True: maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_30__input_53  
FPGA True: avgpool2d  VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0  
FPGA True: mul        VGG__VGG_AdaptiveAvgPool2d_avgpool__1544  
Schedule boost
0 data VGG__input_0 False 1
1 conv2d VGG__VGG_Sequential_features__Conv2d_0__input_3 True 1
2 conv2d VGG__VGG_Sequential_features__Conv2d_2__input_7 True 1
3 maxpool2d VGG__VGG_Sequential_features__MaxPool2d_4__input_9 True 1
4 conv2d VGG__VGG_Sequential_features__Conv2d_5__input_11 True 1
5 conv2d VGG__VGG_Sequential_features__Conv2d_7__input_15 True 1
6 maxpool2d VGG__VGG_Sequential_features__MaxPool2d_9__input_17 True 1
7 conv2d VGG__VGG_Sequential_features__Conv2d_10__input_19 True 1
8 conv2d VGG__VGG_Sequential_features__Conv2d_12__input_23 True 1
9 conv2d VGG__VGG_Sequential_features__Conv2d_14__input_27 True 1
10 maxpool2d VGG__VGG_Sequential_features__MaxPool2d_16__input_29 True 1
11 conv2d VGG__VGG_Sequential_features__Conv2d_17__input_31 True 1
12 conv2d VGG__VGG_Sequential_features__Conv2d_19__input_35 True 1
13 conv2d VGG__VGG_Sequential_features__Conv2d_21__input_39 True 1
14 maxpool2d VGG__VGG_Sequential_features__MaxPool2d_23__input_41 True 1
15 conv2d VGG__VGG_Sequential_features__Conv2d_24__input_43 True 1
16 conv2d VGG__VGG_Sequential_features__Conv2d_26__input_47 True 1
17 conv2d VGG__VGG_Sequential_features__Conv2d_28__input_51 True 1
18 maxpool2d VGG__VGG_Sequential_features__MaxPool2d_30__input_53 True 1
19 avgpool2d VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 True 1
20 mul VGG__VGG_AdaptiveAvgPool2d_avgpool__1544 True 1
Outputs ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544']
Inputs  ['VGG__input_0']
Floyd & Warshall
BFS
 ############################################# 
 ######  Final graph
 ############################################# 
  0 data       VGG__input_0 Ops 0 Shape [1, 224, 224, 3]  IN [] OUT ['VGG__VGG_Sequential_features__Conv2d_0__input_3']
  1 conv2d     VGG__VGG_Sequential_features__Conv2d_0__input_3 Ops 86704128 Shape [1, 224, 224, 64]  IN ['VGG__input_0'] OUT ['VGG__VGG_Sequential_features__Conv2d_2__input_7']
  2 conv2d     VGG__VGG_Sequential_features__Conv2d_2__input_7 Ops 1849688064 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__Conv2d_0__input_3'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9']
  3 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_4__input_9 Ops 3211264 Shape [1, 112, 112, 64]  IN ['VGG__VGG_Sequential_features__Conv2d_2__input_7'] OUT ['VGG__VGG_Sequential_features__Conv2d_5__input_11']
  4 conv2d     VGG__VGG_Sequential_features__Conv2d_5__input_11 Ops 924844032 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9'] OUT ['VGG__VGG_Sequential_features__Conv2d_7__input_15']
  5 conv2d     VGG__VGG_Sequential_features__Conv2d_7__input_15 Ops 1849688064 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__Conv2d_5__input_11'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17']
  6 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_9__input_17 Ops 1605632 Shape [1, 56, 56, 128]  IN ['VGG__VGG_Sequential_features__Conv2d_7__input_15'] OUT ['VGG__VGG_Sequential_features__Conv2d_10__input_19']
  7 conv2d     VGG__VGG_Sequential_features__Conv2d_10__input_19 Ops 924844032 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17'] OUT ['VGG__VGG_Sequential_features__Conv2d_12__input_23']
  8 conv2d     VGG__VGG_Sequential_features__Conv2d_12__input_23 Ops 1849688064 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_10__input_19'] OUT ['VGG__VGG_Sequential_features__Conv2d_14__input_27']
  9 conv2d     VGG__VGG_Sequential_features__Conv2d_14__input_27 Ops 1849688064 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_12__input_23'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29']
 10 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_16__input_29 Ops 802816 Shape [1, 28, 28, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_14__input_27'] OUT ['VGG__VGG_Sequential_features__Conv2d_17__input_31']
 11 conv2d     VGG__VGG_Sequential_features__Conv2d_17__input_31 Ops 924844032 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29'] OUT ['VGG__VGG_Sequential_features__Conv2d_19__input_35']
 12 conv2d     VGG__VGG_Sequential_features__Conv2d_19__input_35 Ops 1849688064 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_17__input_31'] OUT ['VGG__VGG_Sequential_features__Conv2d_21__input_39']
 13 conv2d     VGG__VGG_Sequential_features__Conv2d_21__input_39 Ops 1849688064 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_19__input_35'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41']
 14 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_23__input_41 Ops 401408 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_21__input_39'] OUT ['VGG__VGG_Sequential_features__Conv2d_24__input_43']
 15 conv2d     VGG__VGG_Sequential_features__Conv2d_24__input_43 Ops 462422016 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41'] OUT ['VGG__VGG_Sequential_features__Conv2d_26__input_47']
 16 conv2d     VGG__VGG_Sequential_features__Conv2d_26__input_47 Ops 462422016 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_24__input_43'] OUT ['VGG__VGG_Sequential_features__Conv2d_28__input_51']
 17 conv2d     VGG__VGG_Sequential_features__Conv2d_28__input_51 Ops 462422016 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_26__input_47'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53']
 18 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_30__input_53 Ops 100352 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_28__input_51'] OUT ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0']
 19 avgpool2d  VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53'] OUT ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544']
 20 mul        VGG__VGG_AdaptiveAvgPool2d_avgpool__1544 Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0'] OUT []
 ############################################# 
 ######  Bilinear: NEAREST + AVGPOOL
 ############################################# 
 ############################################# 
 ######  Inner Products -> Conv
 ############################################# 
 ############################################# 
 ######  Scale -> Conv
 ############################################# 
 avgpool2d VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 *MULT  VGG__VGG_AdaptiveAvgPool2d_avgpool__1544 
 ############################################# 
 ######   Concat of concat, ELt + Concat, Any -> 2 concats
 ############################################# 
 ############################################# 
 ######   Holes Treatments
 ############################################# 
 ############################################# 
 ######  WEIGHT & BIAS into Tensors
 ############################################# 
 ############################################# 
 ######  Dilated convolution
 ############################################# 
 ############################################# 
 ######  Depth Wise as Group convolution
 ############################################# 
 ############################################# 
 ######  Depth Wise as Full convolution
 ############################################# 
 ############################################# 
 ######  DownSample + Conv -> Conv + stride
 ############################################# 
Floyd & Warshall
BFS
 ############################################# 
 ######  topological schedule BFS
 ############################################# 
 ############################################# 
 ######  topological DFS
 ############################################# 
DFS_t VGG__input_0
 ############################################# 
 ######  TFS
 ############################################# 
 ############################################# 
 ######  INC
 ############################################# 
INC
 ############################################# 
 ######  Singleton
 ############################################# 
 ############################################# 
 ######  AIE2 density
 ############################################# 
 ############################################# 
 ######  AIE2 Sub Volume Computation
 ############################################# 
VGG__input_0 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__input_0 W:            None	I:            None	O:            None	D:1.000000	T:data
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_0__input_3 I TensorShapes(batch=1, height=224, width=224, channel=3) O TensorShapes(batch=1, height=224, width=224, channel=64)
VGG__VGG_Sequential_features__Conv2d_0__input_3 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_0__input_3 W:   [16, 3, 3, 8]	I:   [1, 4, 47, 8]	O:  [1, 2, 45, 64]	D:0.333333	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_0__input_3_franky (5, 1, 1220) TensorShapes(batch=5, height=1, width=1220, channel=1)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_2__input_7 I TensorShapes(batch=1, height=224, width=224, channel=64) O TensorShapes(batch=1, height=224, width=224, channel=64)
VGG__VGG_Sequential_features__Conv2d_2__input_7 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_2__input_7 W:  [16, 3, 3, 64]	I:  [1, 4, 45, 32]	O:  [1, 2, 45, 64]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_2__input_7_franky (5, 1, 9312) TensorShapes(batch=5, height=1, width=9312, channel=1)
#### This is sub computation at layer level VGG__VGG_Sequential_features__MaxPool2d_4__input_9 I TensorShapes(batch=1, height=224, width=224, channel=64) O TensorShapes(batch=1, height=112, width=112, channel=64)
VGG__VGG_Sequential_features__MaxPool2d_4__input_9 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_4__input_9 W:            None	I:  [1, 4, 46, 64]	O:  [1, 2, 23, 64]	D:1.000000	T:maxpool2d
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_5__input_11 I TensorShapes(batch=1, height=112, width=112, channel=64) O TensorShapes(batch=1, height=112, width=112, channel=128)
CIN [64, 32, 16, 8]
COUT [32, 16, 8]
VGG__VGG_Sequential_features__Conv2d_5__input_11 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  1 CIN  0
VGG__VGG_Sequential_features__Conv2d_5__input_11 W:  [16, 3, 3, 64]	I:  [1, 4, 25, 64]	O: [1, 2, 23, 128]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_5__input_11_franky (8, 1, 9312) TensorShapes(batch=8, height=1, width=9312, channel=1)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_7__input_15 I TensorShapes(batch=1, height=112, width=112, channel=128) O TensorShapes(batch=1, height=112, width=112, channel=128)
CIN [128, 64, 32, 16, 8]
COUT [32, 16, 8]
VGG__VGG_Sequential_features__Conv2d_7__input_15 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  1 CIN  0
VGG__VGG_Sequential_features__Conv2d_7__input_15 W:  [8, 3, 3, 128]	I:  [1, 4, 23, 64]	O: [1, 2, 23, 128]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_7__input_15_franky (16, 1, 9280) TensorShapes(batch=16, height=1, width=9280, channel=1)
#### This is sub computation at layer level VGG__VGG_Sequential_features__MaxPool2d_9__input_17 I TensorShapes(batch=1, height=112, width=112, channel=128) O TensorShapes(batch=1, height=56, width=56, channel=128)
VGG__VGG_Sequential_features__MaxPool2d_9__input_17 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_9__input_17 W:            None	I: [1, 4, 24, 128]	O: [1, 2, 12, 128]	D:1.000000	T:maxpool2d
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_10__input_19 I TensorShapes(batch=1, height=56, width=56, channel=128) O TensorShapes(batch=1, height=56, width=56, channel=256)
CIN [128, 64, 32, 16, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_10__input_19 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_10__input_19 W:  [56, 3, 3, 16]	I: [1, 4, 14, 128]	O: [1, 2, 12, 256]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_10__input_19_franky (5, 8, 8316) TensorShapes(batch=5, height=1, width=8316, channel=8)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_12__input_23 I TensorShapes(batch=1, height=56, width=56, channel=256) O TensorShapes(batch=1, height=56, width=56, channel=256)
CIN [256, 128, 64, 32, 16, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_12__input_23 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_12__input_23 W:  [56, 3, 3, 16]	I: [1, 4, 12, 128]	O: [1, 2, 12, 256]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_12__input_23_franky (5, 16, 8316) TensorShapes(batch=5, height=1, width=8316, channel=16)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_14__input_27 I TensorShapes(batch=1, height=56, width=56, channel=256) O TensorShapes(batch=1, height=56, width=56, channel=256)
CIN [256, 128, 64, 32, 16, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
COUT [56, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_14__input_27 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_14__input_27 W:  [56, 3, 3, 16]	I: [1, 4, 12, 128]	O: [1, 2, 12, 256]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_14__input_27_franky (5, 16, 8316) TensorShapes(batch=5, height=1, width=8316, channel=16)
#### This is sub computation at layer level VGG__VGG_Sequential_features__MaxPool2d_16__input_29 I TensorShapes(batch=1, height=56, width=56, channel=256) O TensorShapes(batch=1, height=28, width=28, channel=256)
VGG__VGG_Sequential_features__MaxPool2d_16__input_29 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_16__input_29 W:            None	I: [1, 4, 12, 256]	O:  [1, 2, 6, 256]	D:1.000000	T:maxpool2d
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_17__input_31 I TensorShapes(batch=1, height=28, width=28, channel=256) O TensorShapes(batch=1, height=28, width=28, channel=512)
CIN [256, 128, 64, 32, 16, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_17__input_31 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_17__input_31 W: [104, 3, 3, 16]	I:  [1, 4, 8, 256]	O:  [1, 2, 6, 512]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_17__input_31_franky (5, 16, 15444) TensorShapes(batch=5, height=1, width=15444, channel=16)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_19__input_35 I TensorShapes(batch=1, height=28, width=28, channel=512) O TensorShapes(batch=1, height=28, width=28, channel=512)
CIN [512, 256, 128, 64, 32, 16, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_19__input_35 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_19__input_35 W: [104, 3, 3, 16]	I:  [1, 4, 6, 256]	O:  [1, 2, 6, 512]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_19__input_35_franky (5, 32, 15444) TensorShapes(batch=5, height=1, width=15444, channel=32)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_21__input_39 I TensorShapes(batch=1, height=28, width=28, channel=512) O TensorShapes(batch=1, height=28, width=28, channel=512)
CIN [512, 256, 128, 64, 32, 16, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_21__input_39 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_21__input_39 W: [104, 3, 3, 16]	I:  [1, 4, 6, 256]	O:  [1, 2, 6, 512]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_21__input_39_franky (5, 32, 15444) TensorShapes(batch=5, height=1, width=15444, channel=32)
#### This is sub computation at layer level VGG__VGG_Sequential_features__MaxPool2d_23__input_41 I TensorShapes(batch=1, height=28, width=28, channel=512) O TensorShapes(batch=1, height=14, width=14, channel=512)
VGG__VGG_Sequential_features__MaxPool2d_23__input_41 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_23__input_41 W:            None	I:  [1, 4, 6, 512]	O:  [1, 2, 3, 512]	D:1.000000	T:maxpool2d
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_24__input_43 I TensorShapes(batch=1, height=14, width=14, channel=512) O TensorShapes(batch=1, height=14, width=14, channel=512)
CIN [512, 256, 128, 64, 32, 16, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_24__input_43 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_24__input_43 W: [104, 3, 3, 16]	I:  [1, 4, 3, 256]	O:  [1, 2, 3, 512]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_24__input_43_franky (5, 32, 15444) TensorShapes(batch=5, height=1, width=15444, channel=32)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_26__input_47 I TensorShapes(batch=1, height=14, width=14, channel=512) O TensorShapes(batch=1, height=14, width=14, channel=512)
CIN [512, 256, 128, 64, 32, 16, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_26__input_47 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_26__input_47 W: [104, 3, 3, 16]	I:  [1, 4, 3, 256]	O:  [1, 2, 3, 512]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_26__input_47_franky (5, 32, 15444) TensorShapes(batch=5, height=1, width=15444, channel=32)
#### This is sub computation at layer level VGG__VGG_Sequential_features__Conv2d_28__input_51 I TensorShapes(batch=1, height=14, width=14, channel=512) O TensorShapes(batch=1, height=14, width=14, channel=512)
CIN [512, 256, 128, 64, 32, 16, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
COUT [104, 8]
weight input channel splitting in core memory
VGG__VGG_Sequential_features__Conv2d_28__input_51 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_28__input_51 W: [104, 3, 3, 16]	I:  [1, 4, 3, 256]	O:  [1, 2, 3, 512]	D:0.125000	T:conv2d
UNIFIED PARAMETERS
unified VGG__VGG_Sequential_features__Conv2d_28__input_51_franky (5, 32, 15444) TensorShapes(batch=5, height=1, width=15444, channel=32)
#### This is sub computation at layer level VGG__VGG_Sequential_features__MaxPool2d_30__input_53 I TensorShapes(batch=1, height=14, width=14, channel=512) O TensorShapes(batch=1, height=7, width=7, channel=512)
VGG__VGG_Sequential_features__MaxPool2d_30__input_53 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_30__input_53 W:            None	I:  [1, 8, 4, 512]	O:  [1, 4, 2, 512]	D:1.000000	T:maxpool2d
#### This is sub computation at layer level VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 I TensorShapes(batch=1, height=7, width=7, channel=512) O TensorShapes(batch=1, height=7, width=7, channel=512)
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 W:            None	I:  [1, 7, 2, 512]	O:  [1, 7, 2, 512]	D:1.000000	T:avgpool2d
Schedule boost
VGG__VGG_Sequential_features__Conv2d_0__input_3 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_0__input_3 WC:   [16, 3, 3, 8]	IC:   [1, 4, 47, 8]	OC:  [1, 2, 45, 64] WM:   [16, 3, 3, 8]	IM: [1, 224, 45, 8]	OM:[1, 224, 45, 64]	D:0.333333	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_2__input_7 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_2__input_7 WC:  [16, 3, 3, 64]	IC:  [1, 4, 45, 32]	OC:  [1, 2, 45, 64] WM:  [16, 3, 3, 64]	IM:[1, 224, 45, 64]	OM:[1, 224, 45, 64]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__MaxPool2d_4__input_9 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_4__input_9 WC:            None	IC:  [1, 4, 46, 64]	OC:  [1, 2, 23, 64] WM:            None	IM:[1, 224, 45, 64]	OM:[1, 112, 23, 64]	D:1.000000	T:maxpool2d	
VGG__VGG_Sequential_features__Conv2d_5__input_11 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  1 CIN  0
VGG__VGG_Sequential_features__Conv2d_5__input_11 WC:  [16, 3, 3, 64]	IC:  [1, 4, 25, 64]	OC: [1, 2, 23, 128] WM:  [32, 3, 3, 64]	IM:[1, 112, 23, 64]	OM:[1, 112, 23, 128]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_7__input_15 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  1 CIN  0
VGG__VGG_Sequential_features__Conv2d_7__input_15 WC:  [8, 3, 3, 128]	IC:  [1, 4, 23, 64]	OC: [1, 2, 23, 128] WM: [32, 3, 3, 128]	IM:[1, 112, 23, 128]	OM:[1, 112, 23, 128]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__MaxPool2d_9__input_17 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_9__input_17 WC:            None	IC: [1, 4, 24, 128]	OC: [1, 2, 12, 128] WM:            None	IM:[1, 112, 23, 128]	OM:[1, 56, 12, 128]	D:1.000000	T:maxpool2d	
VGG__VGG_Sequential_features__Conv2d_10__input_19 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_10__input_19 WC:  [56, 3, 3, 16]	IC: [1, 4, 14, 128]	OC: [1, 2, 12, 256] WM: [56, 3, 3, 128]	IM:[1, 56, 12, 128]	OM:[1, 56, 12, 256]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_12__input_23 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_12__input_23 WC:  [56, 3, 3, 16]	IC: [1, 4, 12, 128]	OC: [1, 2, 12, 256] WM: [56, 3, 3, 256]	IM:[1, 56, 12, 256]	OM:[1, 56, 12, 256]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_14__input_27 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_14__input_27 WC:  [56, 3, 3, 16]	IC: [1, 4, 12, 128]	OC: [1, 2, 12, 256] WM: [56, 3, 3, 256]	IM:[1, 56, 12, 256]	OM:[1, 56, 12, 256]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__MaxPool2d_16__input_29 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_16__input_29 WC:            None	IC: [1, 4, 12, 256]	OC:  [1, 2, 6, 256] WM:            None	IM:[1, 56, 12, 256]	OM: [1, 28, 6, 256]	D:1.000000	T:maxpool2d	
VGG__VGG_Sequential_features__Conv2d_17__input_31 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_17__input_31 WC: [104, 3, 3, 16]	IC:  [1, 4, 8, 256]	OC:  [1, 2, 6, 512] WM:[104, 3, 3, 256]	IM: [1, 28, 6, 256]	OM: [1, 28, 6, 512]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_19__input_35 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_19__input_35 WC: [104, 3, 3, 16]	IC:  [1, 4, 6, 256]	OC:  [1, 2, 6, 512] WM:[104, 3, 3, 512]	IM: [1, 28, 6, 512]	OM: [1, 28, 6, 512]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_21__input_39 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_21__input_39 WC: [104, 3, 3, 16]	IC:  [1, 4, 6, 256]	OC:  [1, 2, 6, 512] WM:[104, 3, 3, 512]	IM: [1, 28, 6, 512]	OM: [1, 28, 6, 512]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__MaxPool2d_23__input_41 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_23__input_41 WC:            None	IC:  [1, 4, 6, 512]	OC:  [1, 2, 3, 512] WM:            None	IM: [1, 28, 6, 512]	OM: [1, 14, 3, 512]	D:1.000000	T:maxpool2d	
VGG__VGG_Sequential_features__Conv2d_24__input_43 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_24__input_43 WC: [104, 3, 3, 16]	IC:  [1, 4, 3, 256]	OC:  [1, 2, 3, 512] WM:[104, 3, 3, 512]	IM: [1, 14, 3, 512]	OM: [1, 14, 3, 512]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_26__input_47 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_26__input_47 WC: [104, 3, 3, 16]	IC:  [1, 4, 3, 256]	OC:  [1, 2, 3, 512] WM:[104, 3, 3, 512]	IM: [1, 14, 3, 512]	OM: [1, 14, 3, 512]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__Conv2d_28__input_51 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__Conv2d_28__input_51 WC: [104, 3, 3, 16]	IC:  [1, 4, 3, 256]	OC:  [1, 2, 3, 512] WM:[104, 3, 3, 512]	IM: [1, 14, 3, 512]	OM: [1, 14, 3, 512]	D:0.125000	T:conv2d	
VGG__VGG_Sequential_features__MaxPool2d_30__input_53 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_Sequential_features__MaxPool2d_30__input_53 WC:            None	IC:  [1, 8, 4, 512]	OC:  [1, 4, 2, 512] WM:            None	IM: [1, 14, 3, 512]	OM:  [1, 7, 2, 512]	D:1.000000	T:maxpool2d	
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 MemTiles W  0 COUT  0 CIN  0 CORE     H  0 COUT  0 CIN  0
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 WC:            None	IC:  [1, 7, 2, 512]	OC:  [1, 7, 2, 512] WM:            None	IM:  [1, 7, 2, 512]	OM:  [1, 7, 2, 512]	D:1.000000	T:avgpool2d	
VGG__VGG_Sequential_features__Conv2d_10__input_19 W CORE TensorShapes(batch=56, height=3, width=3, channel=16) MEM TensorShapes(batch=56, height=3, width=3, channel=128)
VGG__VGG_Sequential_features__Conv2d_12__input_23 W CORE TensorShapes(batch=56, height=3, width=3, channel=16) MEM TensorShapes(batch=56, height=3, width=3, channel=256)
VGG__VGG_Sequential_features__Conv2d_14__input_27 W CORE TensorShapes(batch=56, height=3, width=3, channel=16) MEM TensorShapes(batch=56, height=3, width=3, channel=256)
VGG__VGG_Sequential_features__Conv2d_17__input_31 W CORE TensorShapes(batch=104, height=3, width=3, channel=16) MEM TensorShapes(batch=104, height=3, width=3, channel=256)
VGG__VGG_Sequential_features__Conv2d_19__input_35 W CORE TensorShapes(batch=104, height=3, width=3, channel=16) MEM TensorShapes(batch=104, height=3, width=3, channel=512)
VGG__VGG_Sequential_features__Conv2d_21__input_39 W CORE TensorShapes(batch=104, height=3, width=3, channel=16) MEM TensorShapes(batch=104, height=3, width=3, channel=512)
VGG__VGG_Sequential_features__Conv2d_24__input_43 W CORE TensorShapes(batch=104, height=3, width=3, channel=16) MEM TensorShapes(batch=104, height=3, width=3, channel=512)
VGG__VGG_Sequential_features__Conv2d_26__input_47 W CORE TensorShapes(batch=104, height=3, width=3, channel=16) MEM TensorShapes(batch=104, height=3, width=3, channel=512)
VGG__VGG_Sequential_features__Conv2d_28__input_51 W CORE TensorShapes(batch=104, height=3, width=3, channel=16) MEM TensorShapes(batch=104, height=3, width=3, channel=512)
# total number of channel split 9
  0 data       VGG__input_0 Ops 0 Shape [1, 224, 224, 3]  IN [] OUT ['VGG__VGG_Sequential_features__Conv2d_0__input_3']
  1 conv2d     VGG__VGG_Sequential_features__Conv2d_0__input_3 Ops 86704128 Shape [1, 224, 224, 64]  IN ['VGG__input_0'] OUT ['VGG__VGG_Sequential_features__Conv2d_2__input_7']
  2 conv2d     VGG__VGG_Sequential_features__Conv2d_2__input_7 Ops 1849688064 Shape [1, 224, 224, 64]  IN ['VGG__VGG_Sequential_features__Conv2d_0__input_3'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9']
  3 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_4__input_9 Ops 3211264 Shape [1, 112, 112, 64]  IN ['VGG__VGG_Sequential_features__Conv2d_2__input_7'] OUT ['VGG__VGG_Sequential_features__Conv2d_5__input_11']
  4 conv2d     VGG__VGG_Sequential_features__Conv2d_5__input_11 Ops 924844032 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__MaxPool2d_4__input_9'] OUT ['VGG__VGG_Sequential_features__Conv2d_7__input_15']
  5 conv2d     VGG__VGG_Sequential_features__Conv2d_7__input_15 Ops 1849688064 Shape [1, 112, 112, 128]  IN ['VGG__VGG_Sequential_features__Conv2d_5__input_11'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17']
  6 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_9__input_17 Ops 1605632 Shape [1, 56, 56, 128]  IN ['VGG__VGG_Sequential_features__Conv2d_7__input_15'] OUT ['VGG__VGG_Sequential_features__Conv2d_10__input_19']
  7 conv2d     VGG__VGG_Sequential_features__Conv2d_10__input_19 Ops 924844032 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__MaxPool2d_9__input_17'] OUT ['VGG__VGG_Sequential_features__Conv2d_12__input_23']
  8 conv2d     VGG__VGG_Sequential_features__Conv2d_12__input_23 Ops 1849688064 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_10__input_19'] OUT ['VGG__VGG_Sequential_features__Conv2d_14__input_27']
  9 conv2d     VGG__VGG_Sequential_features__Conv2d_14__input_27 Ops 1849688064 Shape [1, 56, 56, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_12__input_23'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29']
 10 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_16__input_29 Ops 802816 Shape [1, 28, 28, 256]  IN ['VGG__VGG_Sequential_features__Conv2d_14__input_27'] OUT ['VGG__VGG_Sequential_features__Conv2d_17__input_31']
 11 conv2d     VGG__VGG_Sequential_features__Conv2d_17__input_31 Ops 924844032 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_16__input_29'] OUT ['VGG__VGG_Sequential_features__Conv2d_19__input_35']
 12 conv2d     VGG__VGG_Sequential_features__Conv2d_19__input_35 Ops 1849688064 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_17__input_31'] OUT ['VGG__VGG_Sequential_features__Conv2d_21__input_39']
 13 conv2d     VGG__VGG_Sequential_features__Conv2d_21__input_39 Ops 1849688064 Shape [1, 28, 28, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_19__input_35'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41']
 14 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_23__input_41 Ops 401408 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_21__input_39'] OUT ['VGG__VGG_Sequential_features__Conv2d_24__input_43']
 15 conv2d     VGG__VGG_Sequential_features__Conv2d_24__input_43 Ops 462422016 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_23__input_41'] OUT ['VGG__VGG_Sequential_features__Conv2d_26__input_47']
 16 conv2d     VGG__VGG_Sequential_features__Conv2d_26__input_47 Ops 462422016 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_24__input_43'] OUT ['VGG__VGG_Sequential_features__Conv2d_28__input_51']
 17 conv2d     VGG__VGG_Sequential_features__Conv2d_28__input_51 Ops 462422016 Shape [1, 14, 14, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_26__input_47'] OUT ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53']
 18 maxpool2d  VGG__VGG_Sequential_features__MaxPool2d_30__input_53 Ops 100352 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__Conv2d_28__input_51'] OUT ['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0']
 19 avgpool2d  VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 Ops 0 Shape [1, 7, 7, 512]  IN ['VGG__VGG_Sequential_features__MaxPool2d_30__input_53'] OUT []
 ############################################# 
 ######  Given a Graph and Schedule boost : We create Live Tensor
 ############################################# 
 ############################################# 
 ######  Reset Live Structure
 ############################################# 
VGG__input_0
VGG__VGG_Sequential_features__Conv2d_0__input_3
VGG__VGG_Sequential_features__Conv2d_2__input_7
VGG__VGG_Sequential_features__MaxPool2d_4__input_9
VGG__VGG_Sequential_features__Conv2d_5__input_11
VGG__VGG_Sequential_features__Conv2d_7__input_15
VGG__VGG_Sequential_features__MaxPool2d_9__input_17
VGG__VGG_Sequential_features__Conv2d_10__input_19
VGG__VGG_Sequential_features__Conv2d_12__input_23
VGG__VGG_Sequential_features__Conv2d_14__input_27
VGG__VGG_Sequential_features__MaxPool2d_16__input_29
VGG__VGG_Sequential_features__Conv2d_17__input_31
VGG__VGG_Sequential_features__Conv2d_19__input_35
VGG__VGG_Sequential_features__Conv2d_21__input_39
VGG__VGG_Sequential_features__MaxPool2d_23__input_41
VGG__VGG_Sequential_features__Conv2d_24__input_43
VGG__VGG_Sequential_features__Conv2d_26__input_47
VGG__VGG_Sequential_features__Conv2d_28__input_51
VGG__VGG_Sequential_features__MaxPool2d_30__input_53
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0
 ############################################# 
 ######  Attempting Code Generation boost
 ############################################# 
 ############################################# 
 ######  Element Wise: reuse one of the operands
 ############################################# 
 ############################################# 
 ######  Concatenation: I love concatenation memory reuse
 ############################################# 
 ############################################# 
 ######  Memory Management given a Schedule
 ############################################# 
Allocating Params in DDR
VGG__VGG_Sequential_features__Conv2d_0__input_3_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 0 0x0 5
VGG__VGG_Sequential_features__Conv2d_0__input_3_franky TensorShapes(batch=5, height=1, width=1220, channel=1) 1536 0xc0 5
VGG__VGG_Sequential_features__Conv2d_2__input_7_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 50688 0x18c0 5
VGG__VGG_Sequential_features__Conv2d_2__input_7_franky TensorShapes(batch=5, height=1, width=9312, channel=1) 52224 0x1980 5
VGG__VGG_Sequential_features__MaxPool2d_4__input_9_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=20) 424960 0xcf80 5
VGG__VGG_Sequential_features__Conv2d_5__input_11_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 425984 0xd000 5
VGG__VGG_Sequential_features__Conv2d_5__input_11_franky TensorShapes(batch=8, height=1, width=9312, channel=1) 427520 0xd0c0 5
VGG__VGG_Sequential_features__Conv2d_7__input_15_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 1023488 0x1f3c0 5
VGG__VGG_Sequential_features__Conv2d_7__input_15_franky TensorShapes(batch=16, height=1, width=9280, channel=1) 1025024 0x1f480 5
VGG__VGG_Sequential_features__MaxPool2d_9__input_17_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=20) 2212864 0x43880 5
VGG__VGG_Sequential_features__Conv2d_10__input_19_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 2213888 0x43900 5
VGG__VGG_Sequential_features__Conv2d_10__input_19_franky TensorShapes(batch=5, height=1, width=8316, channel=8) 2215424 0x439c0 5
VGG__VGG_Sequential_features__Conv2d_12__input_23_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 4876800 0x94d40 5
VGG__VGG_Sequential_features__Conv2d_12__input_23_franky TensorShapes(batch=5, height=1, width=8316, channel=16) 4878336 0x94e00 5
VGG__VGG_Sequential_features__Conv2d_14__input_27_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 10200576 0x1374c0 5
VGG__VGG_Sequential_features__Conv2d_14__input_27_franky TensorShapes(batch=5, height=1, width=8316, channel=16) 10202112 0x137580 5
VGG__VGG_Sequential_features__MaxPool2d_16__input_29_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=20) 15524352 0x1d9c40 5
VGG__VGG_Sequential_features__Conv2d_17__input_31_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 15525376 0x1d9cc0 5
VGG__VGG_Sequential_features__Conv2d_17__input_31_franky TensorShapes(batch=5, height=1, width=15444, channel=16) 15526912 0x1d9d80 5
VGG__VGG_Sequential_features__Conv2d_19__input_35_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 25411072 0x3077c0 5
VGG__VGG_Sequential_features__Conv2d_19__input_35_franky TensorShapes(batch=5, height=1, width=15444, channel=32) 25412608 0x307880 5
VGG__VGG_Sequential_features__Conv2d_21__input_39_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 45180928 0x562d00 5
VGG__VGG_Sequential_features__Conv2d_21__input_39_franky TensorShapes(batch=5, height=1, width=15444, channel=32) 45182464 0x562dc0 5
VGG__VGG_Sequential_features__MaxPool2d_23__input_41_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=20) 64950784 0x7be240 5
VGG__VGG_Sequential_features__Conv2d_24__input_43_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 64951808 0x7be2c0 5
VGG__VGG_Sequential_features__Conv2d_24__input_43_franky TensorShapes(batch=5, height=1, width=15444, channel=32) 64953344 0x7be380 5
VGG__VGG_Sequential_features__Conv2d_26__input_47_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 84721664 0xa19800 5
VGG__VGG_Sequential_features__Conv2d_26__input_47_franky TensorShapes(batch=5, height=1, width=15444, channel=32) 84723200 0xa198c0 5
VGG__VGG_Sequential_features__Conv2d_28__input_51_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=47) 104491520 0xc74d40 5
VGG__VGG_Sequential_features__Conv2d_28__input_51_franky TensorShapes(batch=5, height=1, width=15444, channel=32) 104493056 0xc74e00 5
VGG__VGG_Sequential_features__MaxPool2d_30__input_53_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=20) 124261376 0xed0280 5
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0_inst_parameters TensorShapes(batch=1, height=1, width=1, channel=20) 124262400 0xed0300 5
Allocating Inputs in DDR dict_keys(['VGG__input_0'])
VGG__input_0 124263424 0xed0380
Param Size 0x7681c00
Allocating Outputs in DDR dict_keys(['VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0'])
VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 127474688 0xf32380
Input Size 0x310000
Output Size 0x31000
Allocating SWAP in DDR
C 127675392 0xf38580
Step  data VGG__input_0
Step  VGG__input_0 is an input
WARNING VGG__input_0 data WARNING
Step  conv2d VGG__VGG_Sequential_features__Conv2d_0__input_3
	Memory access IN DDR -- PAR pa  -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__Conv2d_0__input_3 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_2__input_7
	Memory access IN DDR -- PAR pa  -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__Conv2d_2__input_7 
Step  maxpool2d VGG__VGG_Sequential_features__MaxPool2d_4__input_9
	Memory access IN DDR -- PAR pa  -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__MaxPool2d_4__input_9 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_5__input_11
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_5__input_11 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_7__input_15
	Memory access IN DDR -- PAR DDR -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__Conv2d_7__input_15 
Step  maxpool2d VGG__VGG_Sequential_features__MaxPool2d_9__input_17
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__MaxPool2d_9__input_17 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_10__input_19
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_10__input_19 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_12__input_23
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_12__input_23 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_14__input_27
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_14__input_27 
Step  maxpool2d VGG__VGG_Sequential_features__MaxPool2d_16__input_29
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__MaxPool2d_16__input_29 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_17__input_31
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_17__input_31 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_19__input_35
	Memory access IN fm  -- PAR DDR -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_19__input_35 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_21__input_39
	Memory access IN fm  -- PAR DDR -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__Conv2d_21__input_39 
Step  maxpool2d VGG__VGG_Sequential_features__MaxPool2d_23__input_41
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__MaxPool2d_23__input_41 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_24__input_43
	Memory access IN DDR -- PAR DDR -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__Conv2d_24__input_43 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_26__input_47
	Memory access IN DDR -- PAR DDR -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__Conv2d_26__input_47 
Step  conv2d VGG__VGG_Sequential_features__Conv2d_28__input_51
	Memory access IN DDR -- PAR DDR -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_Sequential_features__Conv2d_28__input_51 
Step  maxpool2d VGG__VGG_Sequential_features__MaxPool2d_30__input_53
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT fm  -- AT at  VGG__VGG_Sequential_features__MaxPool2d_30__input_53 
Step  avgpool2d VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0
	Memory access IN fm  -- PAR pa  -- TMP fm  -- OUT DDR -- AT at  VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 
Allocating SWAP in DDR
 ############################################# 
 ######  Naive instruction dependency
 ############################################# 
 ############################################# 
 ######  Code Generation at Node Level and then Recursively
 ############################################# 
Dependency ON 0 0 CUR 0 BY 0 
1 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
1 22 VGG__input_0 ON 0 CUR 0 BY 1
2 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
2 22 VGG__VGG_Sequential_features__Conv2d_0__input_3 ON 0 CUR 4 BY 1
3 22 <class 'SC.HwAbstraction.code_pool.Pool'> False
3 22 VGG__VGG_Sequential_features__Conv2d_2__input_7 ON 2 CUR 4 BY 9
4 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
4 22 VGG__VGG_Sequential_features__MaxPool2d_4__input_9 ON 2 CUR 8 BY 1
5 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
5 22 VGG__VGG_Sequential_features__Conv2d_5__input_11 ON 2 CUR 4 BY 2
6 22 <class 'SC.HwAbstraction.code_pool.Pool'> False
6 22 VGG__VGG_Sequential_features__Conv2d_7__input_15 ON 4 CUR 4 BY 1
7 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
7 22 VGG__VGG_Sequential_features__MaxPool2d_9__input_17 ON 2 CUR 8 BY 1
8 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
8 22 VGG__VGG_Sequential_features__Conv2d_10__input_19 ON 8 CUR 4 BY 1
9 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
9 22 VGG__VGG_Sequential_features__Conv2d_12__input_23 ON 4 CUR 4 BY 1
10 22 <class 'SC.HwAbstraction.code_pool.Pool'> False
10 22 VGG__VGG_Sequential_features__Conv2d_14__input_27 ON 4 CUR 4 BY 9
11 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
11 22 VGG__VGG_Sequential_features__MaxPool2d_16__input_29 ON 4 CUR 8 BY 1
12 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
12 22 VGG__VGG_Sequential_features__Conv2d_17__input_31 ON 8 CUR 4 BY 1
13 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
13 22 VGG__VGG_Sequential_features__Conv2d_19__input_35 ON 4 CUR 4 BY 1
14 22 <class 'SC.HwAbstraction.code_pool.Pool'> False
14 22 VGG__VGG_Sequential_features__Conv2d_21__input_39 ON 4 CUR 4 BY 9
15 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
15 22 VGG__VGG_Sequential_features__MaxPool2d_23__input_41 ON 4 CUR 8 BY 2
16 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
16 22 VGG__VGG_Sequential_features__Conv2d_24__input_43 ON 8 CUR 4 BY 1
17 22 <class 'SC.HwAbstraction.code_convolution.Conv'> False
17 22 VGG__VGG_Sequential_features__Conv2d_26__input_47 ON 2 CUR 4 BY 1
18 22 <class 'SC.HwAbstraction.code_pool.Pool'> False
18 22 VGG__VGG_Sequential_features__Conv2d_28__input_51 ON 2 CUR 4 BY 1
19 22 <class 'SC.HwAbstraction.code_pool.Pool'> False
19 22 VGG__VGG_Sequential_features__MaxPool2d_30__input_53 ON 2 CUR 8 BY 9
20 22 <class 'SC.HwAbstraction.code_end.Bracket_End'> False
20 22 VGG__VGG_AdaptiveAvgPool2d_avgpool__1544_i0 ON 8 CUR 8 BY 2
21 22 bracket ON 2 CUR 2 BY 0
 ############################################# 
 ######  Code Generation at Node Level and then Recursively
 ############################################# 
CODE VGG__VGG_Sequential_features__Conv2d_0__input_3 conv2d
('VGG__input_0', 5)
('VGG__VGG_Sequential_features__Conv2d_0__input_3', 5)
[True, True] True
Resolution Resolution(inputs={'VGG__input_0': 'tiling'}, tmp=None, out={'VGG__VGG_Sequential_features__Conv2d_0__input_3': 'tiling'})
Memory_Access(input=5, parameters=3, output=5)
BATCH IN  Shape [1, 224, 224, 3] Heights [[113, 0], [113, 111]] 
BATCH OUT Shape [1, 224, 224, 64] Heights [[112, 0], [112, 112]] 
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=45, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=45, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
CODE VGG__VGG_Sequential_features__Conv2d_2__input_7 conv2d
('VGG__VGG_Sequential_features__Conv2d_0__input_3', 5)
('VGG__VGG_Sequential_features__Conv2d_2__input_7', 5)
[True, True] True
Resolution Resolution(inputs={'VGG__VGG_Sequential_features__Conv2d_0__input_3': 'tiling'}, tmp=None, out={'VGG__VGG_Sequential_features__Conv2d_2__input_7': 'tiling'})
Memory_Access(input=5, parameters=3, output=5)
BATCH IN  Shape [1, 224, 224, 64] Heights [[57, 0], [58, 55], [58, 111], [57, 167]] 
BATCH OUT Shape [1, 224, 224, 64] Heights [[56, 0], [56, 56], [56, 112], [56, 168]] 
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=45, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=45, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=45, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=45, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
CODE VGG__VGG_Sequential_features__MaxPool2d_4__input_9 maxpool2d
('VGG__VGG_Sequential_features__Conv2d_2__input_7', 5)
('VGG__VGG_Sequential_features__MaxPool2d_4__input_9', 5)
[True, True] True
Resolution Resolution(inputs={'VGG__VGG_Sequential_features__Conv2d_2__input_7': 'tiling'}, tmp=None, out={'VGG__VGG_Sequential_features__MaxPool2d_4__input_9': 'tiling'})
Memory_Access(input=5, parameters=3, output=5)
BATCH IN  Shape [1, 224, 224, 64] Heights [[112, 0], [112, 112]] 
BATCH OUT Shape [1, 112, 112, 64] Heights [[56, 0], [56, 56]] 
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=23, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [64] TensorShapes(batch=1, height=2, width=23, channel=64) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [128] TensorShapes(batch=1, height=2, width=23, channel=128) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
CODE VGG__VGG_Sequential_features__Conv2d_7__input_15 conv2d
('VGG__VGG_Sequential_features__Conv2d_5__input_11', 5)
('VGG__VGG_Sequential_features__Conv2d_7__input_15', 5)
[True, True] True
Resolution Resolution(inputs={'VGG__VGG_Sequential_features__Conv2d_5__input_11': 'tiling'}, tmp=None, out={'VGG__VGG_Sequential_features__Conv2d_7__input_15': 'tiling'})
Memory_Access(input=5, parameters=5, output=5)
 You are HERE because IO and WEIGHTs are all in DDR   And you have to split the space for both so you can tile the hight   This is a reminder 
1484800 513802240 13189120 0.6666666666666666
BATCH IN  Shape [1, 112, 112, 128] Heights [[57, 0], [57, 55]] 
BATCH OUT Shape [1, 112, 112, 128] Heights [[56, 0], [56, 56]] 
PAR BATCH NOT PIPELINED [16] Name VGG__VGG_Sequential_features__Conv2d_7__input_15_franky Parameter True	Space 1187840 bits, Start 1025024 End 2212864 Extra 1 	Specifier 0 Layout 5 	Shape  [16, 1, 9280, 1] CNN_Shape TensorShapes(batch=16, height=1, width=9280, channel=1)
BATCH AIE2 CORES OUTPUT CHANNEL [16] TensorShapes(batch=1, height=2, width=23, channel=128) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
PAR BATCH NOT PIPELINED [16] Name VGG__VGG_Sequential_features__Conv2d_7__input_15_franky Parameter True	Space 1187840 bits, Start 1025024 End 2212864 Extra 1 	Specifier 0 Layout 5 	Shape  [16, 1, 9280, 1] CNN_Shape TensorShapes(batch=16, height=1, width=9280, channel=1)
BATCH AIE2 CORES OUTPUT CHANNEL [16] TensorShapes(batch=1, height=2, width=23, channel=128) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [128] TensorShapes(batch=1, height=2, width=12, channel=128) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [256] TensorShapes(batch=1, height=2, width=12, channel=256) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
ERROR BANKD ADDRESS  Name VGG__VGG_Sequential_features__Conv2d_10__input_19 Parameter False	Space 6881280 bits, Start 5013504 End 11894784 Extra 1 	Specifier 0 Layout 4 	Shape  [1, 56, 56, 256] CNN_Shape TensorShapes(batch=1, height=56, width=56, channel=256)
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [256] TensorShapes(batch=1, height=2, width=12, channel=256) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
ERROR BANKD ADDRESS  Name VGG__VGG_Sequential_features__Conv2d_10__input_19 Parameter False	Space 6881280 bits, Start 4988928 End 11870208 Extra 1 	Specifier 0 Layout 4 	Shape  [1, 56, 56, 256] CNN_Shape TensorShapes(batch=1, height=56, width=56, channel=256)
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [256] TensorShapes(batch=1, height=2, width=12, channel=256) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [256] TensorShapes(batch=1, height=2, width=6, channel=256) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
BATCH AIE2 CORES OUTPUT CHANNEL [512] TensorShapes(batch=1, height=2, width=6, channel=512) 
BATCH AIE2 CORES HEIGHT [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
ERROR BANKD ADDRESS  Name VGG__VGG_Sequential_features__Conv2d_17__input_31 Parameter False	Space 3440640 bits, Start 4734976 End 8175616 Extra 1 	Specifier 0 Layout 4 	Shape  [1, 28, 28, 512] CNN_Shape TensorShapes(batch=1, height=28, width=28, channel=512)
BATCH AIE2 CORES OUTPUT CHANNEL Done 
BATCH AIE2 CORES HEIGHT Done
preferred [1, 1, 1, 1, 1]
PAR BATCH NOT PIPELINED [1, 1, 1, 1, 1] Name VGG__VGG_Sequential_features__Conv2d_19__input_35_franky Parameter True	Space 19768320 bits, Start 25412608 End 45180928 Extra 1 	Specifier 0 Layout 5 	Shape  [5, 1, 15444, 32] CNN_Shape TensorShapes(batch=5, height=1, width=15444, channel=32)
ERROR no MOVE DOWN
> /wrk/hdstaff/paolod/perforce/RDI_paolod_Dev_work/src/DeepLearning/xilinx/gitlab/vai-toolchain/dpuv4sparse-pycompiler/dpuv3-pycompiler/SC/Scheduler/memory.py(416)move_down()
-> bt = self.hw.move_down(BBt)
(Pdb) (Pdb)  ### DANGER: P. learn how to do data management
None
Name VGG__VGG_Sequential_features__Conv2d_19__input_35_franky Parameter True	Space 3953664 bits, Start 25412608 End 29366272 Extra 1 	Specifier 0 Layout 5 	Shape  [5, 1, 15444, 32] CNN_Shape TensorShapes(batch=1, height=1, width=15444, channel=32)
array temp
StandardCore
Per Core Ping (and Pong):
MiscCoreBuffer : 11264-bit InputCoreBuffer : 131072-bit OutputCoreBuffer : 131072-bit WeightCoreBuffer : 131072-bit 
 [StandardCore(0,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(0,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(0,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(0,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(0,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [StandardCore(1,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(1,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(1,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(1,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(1,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [StandardCore(2,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(2,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(2,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(2,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(2,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [StandardCore(3,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(3,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(3,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(3,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(3,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']
 [StandardCore(4,0)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(4,1)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(4,2)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(4,3)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16'] [StandardCore(4,4)  ['-KB-1'] ['-KB-8', '-KB-8'] ['-KB-8', '-KB-8'] ['-KB-16']

Name VGG__VGG_Sequential_features__Conv2d_19__input_35 Type conv2d Composed [] 
	Pred ['VGG__VGG_Sequential_features__Conv2d_17__input_31'] 
	Succ ['VGG__VGG_Sequential_features__Conv2d_21__input_39'] 
	Time in seconds DM 0.000000e+00  MD 0.000000e+00  MC [[0.0, 0]] write CM [] 
> /proj/xsjhdstaff3/paolod/perforce/RDI_paolod_Dev_work/src/DeepLearning/xilinx/gitlab/vai-toolchain/dpuv4sparse-pycompiler/dpuv3-pycompiler/SC/HwAbstraction/code_generation.py(594)my_simplified_code()
-> CCode.extend(c.code_generation(dbon,dbby) )
(Pdb) preferred [1, 1, 1, 1, 1]
PAR BATCH NOT PIPELINED [1, 1, 1, 1, 1] Name VGG__VGG_Sequential_features__Conv2d_19__input_35_franky Parameter True	Space 19768320 bits, Start 25412608 End 45180928 Extra 1 	Specifier 0 Layout 5 	Shape  [5, 1, 15444, 32] CNN_Shape TensorShapes(batch=5, height=1, width=15444, channel=32)
ERROR no MOVE DOWN
> /wrk/hdstaff/paolod/perforce/RDI_paolod_Dev_work/src/DeepLearning/xilinx/gitlab/vai-toolchain/dpuv4sparse-pycompiler/dpuv3-pycompiler/SC/Scheduler/memory.py(416)move_down()
-> bt = self.hw.move_down(BBt)
(Pdb) 