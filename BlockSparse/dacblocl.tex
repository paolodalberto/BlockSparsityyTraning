
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym DAC]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June ,
  2024}{San Francisco, CA}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{TBD} %978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\input{mydef}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Weight Block Sparsity: Training, Compilers, and Accelerators}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Anonymous} 
%\author{Paolo D'\!Alberto}
%\author{Taehee Jeong}
%\author{Akshay Jain}
%\author{Shreyas Manjunath}
%\author{Mrinal Sarmah}
%\author{Samuel Hsu}
%\author{Nitesh Pipralia}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{ D'\!Alberto et al.}
\renewcommand{\shortauthors}{ Anonymous  et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
We present the main ideas about a vertical system where convolution
and matrix multiplication weights can be trained to exploit an 8x8
block sparsity, compilers recognize such a sparsity for both data
compaction and computation splitting into threads. If we take a
Resnet50, we can reduce the weight by half with little accuracy
loss. We can achieve speeds similar to an hypothetical Resnet25. We
shall present performance estimates by accurate and complete code
generation for a small and efficient set of AIE2 (Xilinx/AMD Versal
FPGAs) using Resnet50, Inception V3, and VGG16.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Sparsity, AI, Performance, FPGA, and Tools}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{TBD}
\received[revised]{TBD}
\received[accepted]{TBD}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
\label{sec:introduction}

We shall start presenting what we mean for block sparsity and for a
vertical solution. Block sparsity is an intuitive concept but it is
also a little misunderstood. Take a matrix multiplication as in
Equation \ref{eq:mat}
\begin{equation}
  \label{eq:mat}
  \begin{pmatrix}
    \Vc{C}_0 & \Vc{C}_1 \\
    \Vc{C}_2 & \Vc{C}_3 \\ 
  \end{pmatrix} = 
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 \\
    \Vc{A}_2 & \Vc{A}_3 \\ 
  \end{pmatrix}\\
  \begin{pmatrix}
    \Vc{0}   & \Vc{B}_1 \\
    \Vc{B}_2 & \Vc{0} \\ 
  \end{pmatrix}\\
\end{equation}
This is simply the computation
{\small \begin{equation}
  \Vc{C}_0 = \Vc{A}_{1} \Vc{B}_{2}; \;
  \Vc{C}_1 = \Vc{A}_{0} \Vc{B}_{1}; \;
  \Vc{C}_2 = \Vc{A}_{3} \Vc{B}_{2}; \;
  \Vc{C}_3 = \Vc{A}_{2} \Vc{B}_{1}
\end{equation}}
and in general with proper $\gamma_i$ (i.e., a mask):
\begin{equation}
  \Vc{C}_i = \sum_{k=0}^1 \Vc{A}_{2i+ k} \big(\gamma_{2*k+i} \Vc{B}_{2*k+i}\big)
\end{equation}
Where the matrix $\Vc{B}$ is constant, diagonal, and each submatrix
$\Vc{B_2}$ and $\Vc{B}_1$ can split further down and may have even
smaller zero blocks. In this work, we chose the basic block of
$\Vc{B}_i = 8\times 8$. It is a great starting point for architectures
based on AMD AIE2 products and we support others.  For example,
\begin{equation}
  \Vc{B} = \dot{\sum}_i \gamma_i \Vc{B}_i, \;\; \gamma_i \in \{0,1\} 
\end{equation}

This is a well known data structure in the sparse computation field:
We can use Compress Block Row or Column format (CBR). There are
standard Matrix Sparse-Matrix Multiplication interfaces and algorithms
for CPU and GPUs using this data format (where only one operand is
sparse or both) \cite{rocSPARSE,cuSPARSE}.

We explore training techniques (PyTorch and the Keras).  The most
successful so far is the simplest: we take a pre-trained model, we
compute a $\Gamma$ per layer using a function to determine the blocks
more likely to be zeros (Norm) and then we train the model till
convergence or accuracy achieved. We take the sparse model and we
quantize to 8-bit integer computations by using the Vitis-AI
quantizer. The final model is a Xilinx IR quantized model. See Section
\ref{sec:training}. We have a custom compiler that takes the XIR model
and an abstraction of a connected set of AIE2. See Section
\ref{sec:compiler}. Given the HW and per layer, the compiler computes
the maximum sub-volume computation per core. By heuristics and
following a schedule, it computes a memory allocation in mem-tile for
input, outputs, and weights. It formats the weights so that to exploit
spatial distribution to Mem-tiles and cores into a single compacted
tensor. We generate all the explicit communications between DDR,
MemTile, and cores. Knowing the subproblem sizes per core and the
computation throughput and with a clear specification of what is
executed in parallel: we can estimate the execution time per layer and
of the entire network with an accuracy closer to a simulation.  We
shall use this in order to write time estimates for all parts of the
computation: we shall show estimates for three CNN models, a eight
different AIE designs; see Section \ref{sec:experiments}.

In the following Section \ref{sec:motivation}, we shall start with a
quantitative measure about the advantages of block sparsity in
general.

\section{Block-Sparse Matrix-Matrix Multiplication}
\label{sec:motivation}

 As thought experiment, consider $\Gamma$ and $\Omega$ two appropriate
 0,1 matrices so that for square matrices in $\R^{N\times N}$
\begin{equation}
  \Vc{C} = (\Gamma \Vc{A}) * (\Omega \Vc{B})^t
\end{equation}
More precisely, consider non-zero blocks of size $k\times k$ so that
\begin{equation}
  \Vc{C}_{i*N+j} = \sum_k ( \gamma_{i*N+k} \Vc{A}_{i*N+k} ) (\dot{\omega}_{j*N+k} \dot{\Vc{B}}_{j*N+k})
\end{equation}


Thanks to the sparsity and if we store only non-zeros, then
$\gamma_{i*N+k}$ and $\dot{\omega}_{j*N+k}$ are at the very least
contiguous. There will be a meaningful product to compute if and only
if $\gamma_{i*N+k} =1$ and  $\dot{\omega}_{j*N+k} =1$.  We merge-sort
these vectors.  See how the Sparse Sparse Matrix multiplication using
Coordinate Block Structure (COO) is applied for example in
in Figure \ref{fig:block}. We shall provide software to reproduce this. % \cite{PaoloG2020}.
Now, the quantitative part, assume we want to
achieve a fixed sparsity (i.e., density) of 50\% for a square matrix
of size $N$ and we choose the block size $k \times k$. The larger $k$
is, the smaller the overhead will be.  The relative performance of the
$k^3$ multiplication is better as $k$ get larger because spatial and
temporal locality and optimized code for a constant/parameterized $k$.

\doublefigure{0.80}{1x1.png}{8x8.png}{Block 1x1 and 8x8
  performance}{fig:block}

In Figure \ref{fig:block}, we present two scatter plots: on the
abscissa the effective multiplication-and-addition number, on the
ordinate the performance in GFLOPS, when the sparse matrix with dense
block is 1x1 and 8x8. Given the same problem, we may use more threads
and thus the Jenga effect.  With the same number of effective
operations, the block permits and exploits higher GFLOPS per effective
operation (Float is 2x faster than Double precision and this can be
emphasized further \cite{Gray2017GPUKF,li2023popsparse} and \cite{pmlr-v119-kurtz20a} .



\section{Block Sparsity: Training and Quantization}
\label{sec:training}

In Convolutional Neural Networks, the two main operations are
convolutions/correlations and fully connected layers (matrix
multiplication). The block sparsity we are seeking to deploy is not
naturally recurring and  we must train the network for it.

First, let us clarify block sparsity for convolution weights, then we
clarify our training process. A convolution has a weight tensor in
four dimension: $\Vc{W} \in \R^{c_{out}\times h \times k \times
  c_{in}}$. In the hyperplane of the $h$ and $k$, 
  we can simplify the weight as $\dot{\Vc{W}} \in \R^{c_{out}
  \times c_{in}}$ and block sparsity can be simply described by a mask
$\Gamma\dot{\Vc{W}}$. Although, we speak of a $8\times 8$ of non
zeros, this is in practice a $8\times h\times k\times 8$ block. For
the matrix multiply $h=k=1$, there is no difference from the
previous discussions.


\subsection{Keras}
We shall provide a repository using Keras \cite{chollet2015keras}
where we implements the contents of this section. %\cite{PaoloK2020}.

We target convolutions only and without quantization. The idea of the
framework is simple: we take any model and we create a copy where we
enhance the convolution with a (non-trainable) $\Gamma$. A convolution
will have three parameters (saving the model into a different format).
The forward computation is modified so that the weights used for
convolution are $\Gamma\Vc{W}$. We assume the backward computation
(i.e., gradient) is done automatically from the forward
definition. There is no need to change the bias. For example, we take
Resnet50 from the keras application repository, we start with a
$\Gamma=1$, and we trained one epoch using imagenet repository
\cite{deng2009imagenet}.  The goal is to choose $\Gamma$ so that we
achieve the required sparsity and to have the minimum loss in
accuracy. Using this repository, we can test different ways to train
but we skip for lack of space (incremental, Fisher measure, Hessian,
diagonal Hessian, penalty losses).

\subsection{$\Gamma$ chosen once and full training ahead: PyTorch}
\label{sec:one-mask}
\label{sec:pytorch}
Take a convolution with $\Gamma = 1$ and weights $\Vc{W}$. For each
$\gamma_i$, this will be representative of a block $\Vc{W}_i \in \R^{8
  \times h \times w \times 8} \sim \R^{8\times 8}$. We can choose the
$\Vc{W}_i$ using a measure of importance:
\begin{itemize}
  \item $L_2 = \sqrt{\sum_k w_k^2}$ with $w_k$ elements of $\Vc{W}_i$,
  \item $L_1 = \sum_k |w_k|$,
  \item Variance $\sigma^2 = \frac{1}{64}\sum_k (w_k -\mu)^2$ with
    $\mu = \frac{1}{64}\sum w_k $ or $\frac{1}{N}\sum w_k$ (the whole
    tensor). In signal processing $\sigma^2$ is the power of the
    signal.
\end{itemize}
We can then sort them in ascending order and for example choose the
first 50\% to set to zero. Then we start re-training. We do this for
the whole network or for one convolution at a time. 

In Table \ref{tab_acc}, we show the results by using one-time masks
and full training: VGG-16, ResNet-50, Inceptionv3 on ImageNet20 (20
classes) and ImageNet1k (1000 classes).  We use three samples per
class for the validation accuracy for ImageNet1k data set; instead, we
use 50 samples per class for ImageNet20. Fine-tuning sparse networks
on the original ImageNet data-set \cite{deng2009imagenet} is
expensive. To reduce the training time, we chose 20 classes (from the
original 1000 classes) with the least number of images per class in
the training data-set and this choice will affect the accuracy because
there are fewer samples for re-training.


\begin{table}[ht]
\caption{Accuracies of the sparsity models}
\label{tab_acc}
\begin{center} 
\scalebox{0.9}
{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rule[-1ex]{0pt}{3.5ex}  Model & Dataset & Baseline  & \multicolumn{3}{c|}{Sparsity}\\
\rule[-1ex]{0pt}{3.5ex}  {} & {} & Acc.(\%) & block & ratio (\%) & Acc.(\%)    \\\hline\hline
\rule[-1ex]{0pt}{3.5ex}  Inception-v3 & ImageNet1k & 77.2 & 8x8 & 50 & 75.5  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet1k & 76.7 & 8x8 & 50 & 74.6  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet1k & 70.6 & 8x8 & 50 & 69.7  \\\hline \hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 8x8 & 25 & 95.1  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 8x8 & 50 & 92.0  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 8x8 & 75 & 87.1  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 1x1 & 25 & 96.0  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 1x1 & 50 & 95.6  \\\hline
\rule[-1ex]{0pt}{3.5ex}  ResNet-50 & ImageNet20 & 96.1 & 1x1 & 75 & 93.5  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet20 & 92.0 & 8x8 & 50 & 89.6  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet20 & 92.0 & 1x1 & 50 & 92.3  \\\hline
\rule[-1ex]{0pt}{3.5ex}  VGG-16    & ImageNet20 & 92.0 & 1x1 & 75 & 91.7  \\\hline
\end{tabular}\vspace{-20pt}
}
\end{center}
\end{table}

Classification accuracy on ImageNet1k drops by only 1 - 2\% after
applying 50\% sparsity with a $8\times 8$ block (this is without any
quantization). We experiment with different block shapes such as
$16\times 4$ and $4\times 16$ on ResNet-50, but the accuracy is
slightly worse compared to block of $8\times 8$. Fine-grained sparsity
($1\times 1$ block) does not sacrifice any accuracy (i.e., almost
any).  We use the sparsified models, we quantize them using Vitis AI,
and we shall use it for time estimates (i.e., Section
\ref{sec:experiments}).


\section{The Compiler and its Code generation for AIE}
\label{sec:compiler}
Within our Xilinx/AMD effort, we can take a PyTorch/Keras model,
quantize it using Vitis AI, and create an intermediate representation
that we call Xilinx Intermediate Representation (XIR). This is a graph
where each node is an operation that reads tensors and writes one
tensor. A convolution has one quantized input: we use the tensor
layout format BHWC, the tensors are represented in INT8 with a
position where the fraction starts (power of two scale). It computes a
tensor using the same layout as the input and with a proper scale. The
weights and bias are properties of the convolutions, as such they can
be tailored; the layout of the weight tensor is $COUT\times h \times w
\times CIN$, which is similar to the caffe layout \cite{Caffe}
(different from \cite{tensorflow}). For this project, we are at the
third generation of a compiler for custom architectures.
\begin{comment}
  (previously DPUV1 and DPUV3int8
  \cite{10.11451/3473334,abs-2110-04327}).
\end{comment}

The main difference is the
ability to represent a parameterized block sparsity (block size and
overall sparsity ratios) and the capability to split these tensors
when we split the computation. All weights are statically prepared
into DDR and we move them explicitly towards the inner levels. Inputs
and outputs have designated space in DDR. DDR can and it will be used
for tensors spills.  The memory allocation to Mem-Tile is basically
coloring algorithms and some heuristics. In this architecture, we do
not allow {\em streaming} of neither data or weights (because they
share space in Mem-Tile). In a previous architecture, we could.

\subsection{AIE Hardware abstraction}

\singlefigure{0.70}{AIE.png}{4x4 AIE representation}{fig:aie}

For this presentation, see Figure \ref{fig:aie}, we work with a mesh
of 4x4 AIE2 cores connected by 4 horizontal and 4 vertical
interconnections (we shall presents estimates for square 2x2,
.. $i\times i$ .. 8x8 and rectangular shapes are in the works). Each
core has 8 banks memories for a total 64KB. About 6 banks are used as
input/output/weight buffers and 2 banks are used as temporary space
for kernels. Each core can request and send data to its direct
neighbors (if aware of connection and control). Double buffering as
ping/pong is commonly used for inputs and outputs.

There are four mem-tiles: each 512KB and each can either be connected
to one columns and its direct neighbor column, or it can be connected
to a row and its neighbor. The total amount of space is 2MB. Memtile
is a circular buffer to exploit more flexible allocation, implicitly a
$2 \times 2$ architecture will have one memtile per column and a total
of two (1MB).

A Memtile can broad-cast data per column or per row. We can dedicate a
memtile for weights, one for activations, or we can share. We shall
show mostly shared configurations. To maximize the computation
parallelism, every core will write data per column into memtile: there
are different designs that can be used. We shall explore the case
where Inputs/Outputs/Weights are evenly distributed across mem-tiles
although the distribution is different per tensor.
\begin{comment}
The DDR is connected with two channels to write into each mem-tile and
each mem-tile can use two channels to write into DDR. DDR and Mem-tile
communications are parallel.  The abstraction can be extended to more
complex connections (from $1\times 1$, $8\times 2$, to $8\times 8$ and
more rows of memtiles designed for larger chips). But $4\times 4$ is
representative for the AIE engines you will find in the next
generation of CPU+FPGA chips.
\end{comment}

\subsection{Sub Volume, Data compression and Data Movement}
The computation is split by column (cores columns): each output tensor
is split evenly by width, so each memtile will store different columns
by width, each core will compute different output channels, and the
computation streams computing the height of the tensor by using core
input and output ping/pong. As often as possible, we store the weights
in the core and we reuse them as much as possible (unless we need to
stream the weight instead). The set of cores is a cohort and we always
choose symmetric computations. This means that we do not merge two
operations like convolution and max-pool and give three columns to
convolution and one column to max-pool.

If we have the inputs, output, and weights in memtile, what is the
largest computation we can do in the AIE? The minimum computation is
at least one output channel and one row (by height). If this is not
possible, we try to reduce the size of the width (e.g., shaping the
tensor in memtile by using DDR spills) and we can manage to split the
input channels and to split the weights accordingly and prepare for
accumulattion. We call W-Split the distribution of tensor by columns
in the AIE mesh. We can COUT-split, this will require the partial
transfer of weights (but we can stream by height).  We can CIN-split
when we need to split by input channel, this is the last resort
because it is also the most expensive (accumulation of the
outputs).

At this stage, the subvolume describes the smallest shape of the
weights we need to manage. We compress the weight accordingly to such
a subvolume so that any data movement will always be a multiple of the
subvolume and can be a single load. Such a compress data will have the
same properties whether it is sparse or dense. Of course, sparse data is
smaller and we compute fewer operations.


\subsection{Schedule and memory allocation}
During the scheduling of each layer, we evaluate what can fit in mem
tile. Here, activation and weight tensors share the space. It means
that an input tensor is distributed among the memtiles identified by
one starting address and a final address and so the weights. At each
step the memory allocation will check if we can allocate. If we
cannot, we evict all tensors into DDR and then split/time the
computation. 

At the end of this stage, every tensor will be associated to an
address in memtile or DDR (or both). If there are only DDR addresses,
the compiler will take the basic computation and, by heuristics, will
split the computation (thus the output tensor) by width, output
channel, height, and input channel (non necessarily in this
order). Every computation will have its data movements for the first
two levels of the memory hierarchy. The heuristics have a simple
objective: find the largest problem fitting the memory level.

We use an implementation that takes a recursive approach in tiling: %\cite{abs-2110-04327}
For clarity, $\sum$ is a sum or reduction and
$\dot{\sum}$ is a parallel loop and a W-split can be written as
\begin{equation}
  \Vc{Y} =  Conv(\Vc{X},\Vc{W}) = \dot{\sum}_w
  Conv(\Vc{X}_w,\Vc{W})
\end{equation}
The split is pre-computed as function of the foot print, before and
after each convolution there will be an explicit data movement. At
this stage each input, output, and weights will have addresses
associated with each sub-computation. Then the code generation of each
$Conv(\Vc{X}_w,\Vc{W})$ is independent and, recursively and as needed,
there will be specific splits of the computation accordingly. This is
a tree (i.e, a root with $w$ children instead of a loop with $w$
iterations, we do not have support for loops at this level). If the
convolution has strides and large kernel, each sub-convolution may
have overlap data but defined addresses and data movement if
necessary. For example, computing the output by rows and the weights
are reused.

Note scheduling and memory allocation among layers is a hard problem
and often barely addressed by other systems. We addressed as first
requirements.

\subsection{Code Generation }
The compiler recursively creates a list of operations smaller and
smaller that can actually be executed Mem-Tile to Mem-Tile. In
practice, there is a further decomposition using only AIE cores but it
is completely determined by the subvolume computation. Here, we shall
show how we produce the execution time estimates as in Figure
\ref{fig:singleconvestimate}.  This is the computation of a
convolution with top/bottom padding by height:
\begin{equation}
  \Vc{Y}_{height} =   Conv(\Vc{X}_{h=0}) \dot{+} \dot{\sum}_{h=1}^9
  Conv(\Vc{X}_w) \dot{+} Conv(\Vc{X}_{h=10})
\end{equation}

An important feature of the current system is the concept of {\bf
  iteration} between mem-tile and core: Using locks and chaining them
(locks like in semaphores): We can write a single instruction from the
prospective of a single core (as a SIMD instruction) but driving all
cores at once (ASM-like code) for multiple iterations ($\dot{\sum}_{h=1}^i
Conv(\Vc{X}_w)$):

{\small %footnotesize
\begin{verbatim}
  LOADFM Lock k_0 mem-tile addr core addr iter i
  CONV iteration      i
  WRITEFM Lock k_1 mem-tile addr core addr iter i
\end{verbatim}
} There is an implicit lock (say \verb2k_x2) that is used for the pong
and the system cycles in between locks (\verb2k_x2 and \verb2k_02).
These three operations will be execute a number of iterations {\em i}
and using a ping/pong they will load different slices of data and
compute different slices of data.

In our environment padding is common and we can manage by a custom
load (from memtile to core) and this requires a custom load that will
not be repeated {\small %\footnotesize
\begin{verbatim}
  ## Head top pad < 50 us First comp block
  LOADFM Lock k_0 mem-tile addr_0 core addr iter 1
  CONV iteration 1
  WRITEFM Lock k_1 mem-tile addr_1 core addr iter 1
  ## Body iteration > 50 us < 150 us
  ## k_0 -> k_2 -> k_4 Lock Chain
  LOADFM Lock k_2 mem-tile addr_2 core addr iter 9
  CONV iteration 7
  WRITEFM Lock k_3 mem-tile addr_3 core addr iter 9
  ## tail bottom pad > 150 us Last computation block
  LOADFM Lock k_4 mem-tile addr_4 core addr iter 1
  CONV iteration 1
  WRITEFM Lock k_5 mem-tile addr_5 core addr iter 1
\end{verbatim}
 } See Figure \ref{fig:singleconvestimate} for how this code will play
out in practice. At this stage we have all the information we
need. Per layer, the code generation is a two pass process: first, we
generate code for the all load/store and then we combine them into
chains having dependency so that to be logically correct and as fast as
possible. Take the code above and we introduce a chain information.
\singlefigure{0.8}{singledenseconv.png}{Resnet single convolution
  with padding for 4x4: legend AIE: LOAD Activation DDR to Mem, LOADW
  weights DDR to Mem, LOADFM Activation Mem to AIE2 cores, LOADWM
  weights Mem to AIE2, WRITE Mem to DDR, WRITEFM AIE2 to Mem, COMP
  Computation }{fig:singleconvestimate}


\subsection{Time Estimation}
At this stage, we need to explain how we can capture the execution
time and visualize it as in Figure \ref{fig:singleconvestimate}. %\ref{fig:estimate-dense}.

We start by the Time estimates for DDR to Mem-Tile. We have two
communication types: activations and weights. Per mem-tile there are
two dedicated channels.
\begin{itemize}
 \item If we share activations and weights in the same mem-tile, we
   can use one channel for activations and one for weights. Thus the
   loads from DDR to MEM-tile (LOAD and LOADW) are parallel with a
   bandwidth of 4GB/s. Writes from mem-tile to DDR (WRITE) can use
   both channels (8GB/s). 

 \item If activations and weights go to different mem-tiles (2 and 2),
   each load is parallel and 8GB/s. Writes
   are identical.
\end{itemize}
Although, mem-tile are circular buffers to improve the memory
allocation, the streaming  is not applicable
here. % as \cite{abs-2110-04327}.
   

The Memtile connections with AIE cores can be designed differently. We
assume here a few channels with again 4GB/s bandwidth. One memetile
can broad cast inputs to a cores column (and to the nearest
neighbor). These communications are for activations (LOADFM). One
Memtile can broadcast to rows of cores (or the nearest neighbor),
these are for weights (LOADWM). We assume that the column and row
communications are parallel and each memtile core connection is
parallel.

Every communication with iteration 1 is synchronous and sequential:
the load, convolution, and store is executed one after the other but
every core is independent.  For synchronization and for bookkeeping,
we assume that AIE2 weights communication (from memtile) to core are
synchronous and halting.

Every communication with iteration larger than one, we assume that
load, computation (COMP), and store (WRITEFM) are executed in parallel
and the overall execution time is the maximum of the estimated time
multiplied by the number of iterations.

We estimate the execution time of a subvolume by the number of
operations divided by the maximum number of operations per cycle which
is in our scenario: $4\times 8 \times 8 = 256 $ operations per cycle
and 1GHz frequency. This is obviously a very optimistic
validation. The execution time is a feature of the analysis but for us
the estimate of the communications is more compelling and we can
easily mute the computation contribution. Note however, that sparsity
really reduces the computation time. 

We do not account the wait and synchronization which are necessary to
reprogram the fabric. These are very expensive running on a few
milliseconds.


\subsection{Convolution example}
\singlefigure{0.999}{singlesparseconv.png}{Resnet single convolution
  with padding and sparsity for 4x4 AIE }{fig:singleconvestimate2}
Here and in Figure \ref{fig:singleconvestimate} and
\ref{fig:singleconvestimate2}, we give a full convolution example with
and without sparsity and with padding. In this way we can explain how
the time is really estimated and also how the hardware works in
principle.

It is clear from the Figures above that there are three computations
(COMP). We load the weight and activations once in memtiles. There are
actually one load per memtile for a total of 4 loads per activations
and 4 loads for weights. Because each load is to a different
memtile, they are parallel.  The activation and weights communications
are using two different channels and then are in parallel with 4GB/s
bandwidth.

There is a single load of the weights from mem-tiles to each
core. This is done once and it is blocking (LOADWM) but it can be as
easily made non blocking and parallel to the activations. There is a
computation using padding (top-padding) and you can see the sequential
execution of load to cores (LOADFM), computation (COMP), and write to
memtile (WRITEFM). This computation has iteration 1. There are 9
iterations for three instructions: we can see the load, the
computation, and write in parallel. This is obviously a
simplification; there will be a little load poking out at the
beginning and a writing poking out at the end.  Then we conclude with
the final computation with padding at the bottom of the computation.

In this particular scenario, the computation dominates the execution
time and compression basically cut the execution time by half: from
200 $\mu s$ to 130 $\mu s$. There are convolutions in Resnet that
realize up to $2\times$ performance but also there are convolutions
that are dominated by the read or by the writing, and where sparsity
help only in space saving. We shall relax sparsity requirements for
those convolutions.

%\SSinglefigure{2.0}{R4x4-4sharedsparse.png}{Resnet50 for 4x4 AIE with 50\%
%  sparse weights}{fig:estimate-sparse}

\section{Results}
\label{sec:experiments}
\begin{table}[htb]
  \caption{Execution Time estimates}
  \label{tab_perf}
\begin{center} 
\begin{tabular}{|l|l|l|l|l|}
  \hline
  AIE2 & Model  & Dense sec      & Sparse sec      \\ \hline\hline
  2x2   & Resnet & 2.492347e-02  & 1.582626e-02 \\ \hline
  3x3   &  & 1.269543e-02  & 8.661490e-03 \\ \hline
  4x4   &  &  1.077318e-02 & 7.064918e-03 \\ \hline
  5x5   &  &  failed       & 4.303485e-03 \\ \hline
  6x6   &  &  5.712521e-03 & 4.490127e-03 \\ \hline
  7x7   &  &  4.205991e-03 & 3.212234e-03 \\ \hline
  8x8   &  &  6.376768e-03 & 4.602027e-03 \\ \hline \hline
  2x2   & IncV3  & 4.283837e-02  & 2.440544e-02 \\ \hline
  3x3   &   & 2.386600e-02  & 1.422390e-02 \\ \hline
  4x4   &   &  1.740967e-02 & 1.012540e-02 \\ \hline
  5x5   &   &  9.690552e-03 & failed       \\ \hline
  6x6   &   &  1.063962e-02 & 6.439692e-03 \\ \hline
  7x7   &   &  8.727651e-03 & failed       \\ \hline
  8x8   &   &  9.093276e-03 & 5.666152e-03 \\ \hline \hline
  2x2   & VGG16  & 4.476212e-02  & 2.608593e-02 \\ \hline
  3x3   &   & failed        & 1.002015e-02 \\ \hline
  4x4   &   &  1.371000e-02 & 8.852128e-03 \\ \hline
  5x5   &   &  failed       & 4.336479e-03 \\ \hline
  6x6   &   &  failed       & 5.770197e-03 \\ \hline
  7x7   &   &  7.455440e-03 & 5.288551e-03 \\ \hline
  8x8   &   &  9.203393e-03 & 6.502333e-03 \\ \hline \hline
          
\end{tabular}
\end{center}
\end{table}

In Table \ref{tab_perf}, we present the performance of sparsity
applied to all the convolutions (except the first one) for Resnet 50,
Inception V3, and VGG16.

%\DDoublefigure{0.80}{Iv36x6-6shareddense.png}{Iv36x6-6sharedsparse.png}{Inception V3 for 6x6 AIE
%  with dense (left/top)  weights and sparse (right/bottom) }{fig:incv3-estimate-dense}

When we generate the code for each instruction, we compute the
execution time. So if we inspect the assembly code we will find time
information in the context whether or not each instruction contribute
directly. For data movement to and from DDR and mem-tile, we reduce
the contribution (sum directly). There is no streaming or
communication overlapping, thus the sum.

For mem-tile to and from core communications and core computations, we
create a time series. We explain in the previous section how we
account for the execution time for instruction with and without
iterations. All of this will be an attribute of the layer (node in the
graph computation).  To create a complete time estimate, we just need
to take the schedule of the computation and the graph, visit each node
accordingly to the schedule, write a {\em json} file describing the
time series, then by {\em javascript} we can visualize the time series
using a browser. The Figures in this paper are generated directly.

For a $4\times 4$ AIE set up, Resnet 50 fits in memtile from the
beginning to the last operation (beside the first convolution and this
is by costruction). The estimates advantage by sparsity is almost
completely achieved. We provide with the software also all the
experimental results %in \cite{PaoloK2020} as interactive interface
and showing the compiler output in long form.

Corner cases are represented as failure in Table \ref{tab_perf}. Some
cases is because of inability to break the weight tensor evenly,
sometime is for incorrect data management especially for prime
numbers. These are all issues will be address as the technology
matures. Please, note that VGG16 using 8x8 is slower than 7x7 by using
sparse.  It may happen because a symmetric computation my use too
small sub-volume computations and thus more iterations.

\section{Conclusions and Context}
This is a multifaceted problem and we present a complete solution from
training techniques, compilers, code generation, HW definition, and
time estimations. It is a vertical software system and more complex
than just a prototype.

This could be seen as a quantization and sparsification problem: How
to reduce the footprint of a CNN network. There are post training
techniques that are targeting quantization and unstructured sparsity
\cite{frantar2023gptq} and all the references within. We need to be
more aggressive and training for it (as starting point
\cite{abs-2102-11289}), our sparsity is not really a property of the
model and software can describe it and the hardware can take advantage
(but we do not need to have hardware support at instruction level).

This work stemmed from a collaboration between Xilinx and Numenta and
the idea set forward by the authors in \cite{ahmad2019dense}, where
the model are too dense and there is a lot we can do about
it. 


The difficulty of generate code for complex architectures can be
described and solved in different ways. There are recent attempts of
introducing SW/HW solution for spatial processors like ours
\cite{Huang2021CoSASB,Russo2023MemoryAwareDA,Cai2023InterlayerSS}.
Usually major attention is given only to matrix multiplication and
GPUs \cite{Gray2017GPUKF} \cite{li2023popsparse}, we can work on only static
sparsity at this time.




\begin{comment}
In Table \ref{tab_perf}, we present the estimates of the total
execution time for three networks and seven configurations. We report
also the case where the compiler fails to generate code.  Here, we
present the collection of execution times as images:
\begin{itemize}
  \item Figure \ref{fig:incv3-1} Inception V3 2x2 dense and sparse
  \item Figure \ref{fig:incv3-2} Inception V3 3x3 dense and sparse
  \item Figure \ref{fig:incv3-3} Inception V3 4x4 dense and sparse
  \item Figure \ref{fig:incv3-4} Inception V3 5x5 dense and sparse FAILs
  \item Figure \ref{fig:incv3-5} Inception V3 6x6 dense and sparse 
  \item Figure \ref{fig:incv3-6} Inception V3 8x8 dense and sparse
  \item Figure \ref{fig:incv3-7} Resnet 50 2x2 dense and sparse   
  \item Figure \ref{fig:incv3-8} Resnet 50 3x3 dense and sparse   
  \item Figure \ref{fig:incv3-9} Resnet 50 4x4 dense and sparse   
  \item Figure \ref{fig:incv3-10} Resnet 50 5x5 dense FAILs and sparse   
  \item Figure \ref{fig:incv3-11} Resnet 50 6x6 dense and sparse   
  \item Figure \ref{fig:incv3-12} VGG16  2x2 dense and sparse   
  \item Figure \ref{fig:incv3-13} VGG16  4x4 dense and sparse   
  \item Figure \ref{fig:incv3-14} VGG16  8x8 dense and sparse   
\end{itemize}
\newpage 
\DDoublefigure{1.3}{IncV3-2x2-2share-dense.png}{IncV3-2x2-2share-sparse.png}{Inception
  for 2x2 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-1}
\DDoublefigure{1.3}{IncV3-3x3-3share-dense.png}{IncV3-3x3-3share-sparse.png}{Inception
  for 3x3 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-2}
\DDoublefigure{1.3}{IncV3-4x4-4share-dense.png}{IncV3-4x4-4share-sparse.png}{Inception
  for 4x4 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-3}
\SSinglefigure{2.6}{IncV3-5x5-5share-dense.png}{Inception for 5x5 AIE
  with dense weights and FAILED for sparse weights }{fig:incv3-4}
\DDoublefigure{1.3}{IncV3-6x6-6share-dense.png}{IncV3-6x6-6share-sparse.png}{Inception
  for 6x6 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-5}
\DDoublefigure{1.3}{IncV3-8x8-8share-dense.png}{IncV3-8x8-8share-sparse.png}{Inception
  for 8x8 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-6}
\DDoublefigure{1.3}{Res2x2-2share-dense.png}{Res2x2-2share-sparse.png}{Resnet
  for 2x2 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-7}
\DDoublefigure{1.3}{Res3x3-3share-dense.png}{Res3x3-3share-sparse.png}{Resnet
  for 3x3 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-8}
\DDoublefigure{1.3}{Res4x4-4share-dense.png}{Res4x4-4share-sparse.png}{Resnet
  for 4x4 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-9}
\SSinglefigure{2.6}{Res5x5-5share-sparse.png}{Resnet for 5x5 AIE with
  sparse weights and FAILED for dense weights}{fig:incv3-10}
\DDoublefigure{1.3}{Res6x6-6share-dense.png}{Res6x6-6share-sparse.png}{Resnet
  for 6x6 AIE with dense (left/top) weights and sparse (right/bottom)
}{fig:incv3-11}

\DDoublefigure{0.9}{VGG162x2-2shared-dense.png}{VGG162x2-2shared-sparse.png}{VGG 16 (No FC)  for 2x2 AIE with dense (left/top) weights and sparse (right/bottom)}{fig:incv3-12}
\DDoublefigure{0.9}{VGG164x4-4shared-dense.png}{VGG164x4-4shared-sparse.png}{VGG 16 (No FC)  for 4x4 AIE with dense (left/top) weights and sparse (right/bottom)}{fig:incv3-13}
\DDoublefigure{0.9}{VGG168x8-8shared-dense.png}{VGG168x8-8shared-sparse.png}{VGG 16 (No FC)  for 8x8 AIE with dense (left/top) weights and sparse (right/bottom)}{fig:incv3-14}
\end{comment}




%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{ref}



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
